<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="https://gmpg.org/xfn/11" rel="profile" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" />

  <title>
    
      Year Prediction by Movie Posters using CNN - 3 &middot; Data Flows in You
    
  </title>

  


  <!-- CSS -->
  <link rel="stylesheet" href="/assets/css/main.css" />
  

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface" />

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://5hyunkwon.github.io/favicon.png" />
<link rel="shortcut icon" href="https://5hyunkwon.github.io/favicon.png" />

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/feed.xml" />

  <!-- Additional head bits without overriding original head -->
</head>


  <body class="post">

    <div id="sidebar">
  <header>
    <div class="site-title">
      <a href="/">
        
          <span class="back-arrow icon"><svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
  <path d="M0 0h24v24H0z" fill="none"/>
  <path d="M20 11H7.83l5.59-5.59L12 4l-8 8 8 8 1.41-1.41L7.83 13H20v-2z"/>
</svg></span>
        
        Data Flows in You
      </a>
    </div>
    <p class="lead"><a href="https://en.wikipedia.org/wiki/Data_science" target="_blank">Data Scientist</a>: The Sexiest Job of the 21st Century</p>
  </header>
  <nav id="sidebar-nav-links">
  
    <a class="home-link "
        href="/">Home</a>
  
  

  

  


  
    
  

  
    
      <a class="page-link "
          href="/about.html">About</a>
    
  

  
    
  

  
    
  

  

  
    
  

  
    
  

  

  
    
  

  
    
  

  
    
  


  


  
    
  

  
    
  

  
    
      <a class="category-link "
          href="/category/cv.html">Computer Vision</a>
    
  

  
    
  

  

  
    
  

  
    
  

  

  
    
  

  
    
  

  
    
  


  <!-- Optional additional links to insert in sidebar nav -->
</nav>


  

  <nav id="sidebar-icon-links">
  
  
  
  

  
    <a id="tags-link"
       class="icon"
       title="Tags" aria-label="Tags"
       href="/tags.html">
      <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M0 0h24v24H0z" fill="none"/>
    <path d="M17.63 5.84C17.27 5.33 16.67 5 16 5L5 5.01C3.9 5.01 3 5.9 3 7v10c0 1.1.9 1.99 2 1.99L16 19c.67 0 1.27-.33 1.63-.84L22 12l-4.37-6.16z"/>
</svg>
    </a>
  

  <a id="github-link"
    class="icon" title="Github" aria-label="Github"
    href="https://github.com/5hyunkwon"
    target="_blank">
    <svg version="1.1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 28" height="24" width="28"><path d="M12 2c6.625 0 12 5.375 12 12 0 5.297-3.437 9.797-8.203 11.391-0.609 0.109-0.828-0.266-0.828-0.578 0-0.391 0.016-1.687 0.016-3.297 0-1.125-0.375-1.844-0.812-2.219 2.672-0.297 5.484-1.313 5.484-5.922 0-1.313-0.469-2.375-1.234-3.219 0.125-0.313 0.531-1.531-0.125-3.187-1-0.313-3.297 1.234-3.297 1.234-0.953-0.266-1.984-0.406-3-0.406s-2.047 0.141-3 0.406c0 0-2.297-1.547-3.297-1.234-0.656 1.656-0.25 2.875-0.125 3.187-0.766 0.844-1.234 1.906-1.234 3.219 0 4.594 2.797 5.625 5.469 5.922-0.344 0.313-0.656 0.844-0.766 1.609-0.688 0.313-2.438 0.844-3.484-1-0.656-1.141-1.844-1.234-1.844-1.234-1.172-0.016-0.078 0.734-0.078 0.734 0.781 0.359 1.328 1.75 1.328 1.75 0.703 2.141 4.047 1.422 4.047 1.422 0 1 0.016 1.937 0.016 2.234 0 0.313-0.219 0.688-0.828 0.578-4.766-1.594-8.203-6.094-8.203-11.391 0-6.625 5.375-12 12-12zM4.547 19.234c0.031-0.063-0.016-0.141-0.109-0.187-0.094-0.031-0.172-0.016-0.203 0.031-0.031 0.063 0.016 0.141 0.109 0.187 0.078 0.047 0.172 0.031 0.203-0.031zM5.031 19.766c0.063-0.047 0.047-0.156-0.031-0.25-0.078-0.078-0.187-0.109-0.25-0.047-0.063 0.047-0.047 0.156 0.031 0.25 0.078 0.078 0.187 0.109 0.25 0.047zM5.5 20.469c0.078-0.063 0.078-0.187 0-0.297-0.063-0.109-0.187-0.156-0.266-0.094-0.078 0.047-0.078 0.172 0 0.281s0.203 0.156 0.266 0.109zM6.156 21.125c0.063-0.063 0.031-0.203-0.063-0.297-0.109-0.109-0.25-0.125-0.313-0.047-0.078 0.063-0.047 0.203 0.063 0.297 0.109 0.109 0.25 0.125 0.313 0.047zM7.047 21.516c0.031-0.094-0.063-0.203-0.203-0.25-0.125-0.031-0.266 0.016-0.297 0.109s0.063 0.203 0.203 0.234c0.125 0.047 0.266 0 0.297-0.094zM8.031 21.594c0-0.109-0.125-0.187-0.266-0.172-0.141 0-0.25 0.078-0.25 0.172 0 0.109 0.109 0.187 0.266 0.172 0.141 0 0.25-0.078 0.25-0.172zM8.937 21.438c-0.016-0.094-0.141-0.156-0.281-0.141-0.141 0.031-0.234 0.125-0.219 0.234 0.016 0.094 0.141 0.156 0.281 0.125s0.234-0.125 0.219-0.219z"></path>
</svg>

  </a>

  <!-- Optional additional links to insert for icons links -->
<a id="facebook-link"
   class="icon" title="Facebook" aria-label="Facebook"
   href="https://www.facebook.com/5hyunkwon"
   target="_blank">
   <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M22.675 0h-21.35c-.732 0-1.325.593-1.325 1.325v21.351c0 .731.593 1.324 1.325 1.324h11.495v-9.294h-3.128v-3.622h3.128v-2.671c0-3.1 1.893-4.788 4.659-4.788 1.325 0 2.463.099 2.795.143v3.24l-1.918.001c-1.504 0-1.795.715-1.795 1.763v2.313h3.587l-.467 3.622h-3.12v9.293h6.116c.73 0 1.323-.593 1.323-1.325v-21.35c0-.732-.593-1.325-1.325-1.325z"/></svg>
</a>

<a id="instagram-link"
   class="icon" title="Instagram" aria-label="Instagram"
   href="https://www.instagram.com/5hyun_kwon/"
   target="_blank">
   <svg xmlns="http://www.w3.org/2000/svg" data-name="Layer 1" viewBox="0 0 24 24"><path d="M17.34,5.46h0a1.2,1.2,0,1,0,1.2,1.2A1.2,1.2,0,0,0,17.34,5.46Zm4.6,2.42a7.59,7.59,0,0,0-.46-2.43,4.94,4.94,0,0,0-1.16-1.77,4.7,4.7,0,0,0-1.77-1.15,7.3,7.3,0,0,0-2.43-.47C15.06,2,14.72,2,12,2s-3.06,0-4.12.06a7.3,7.3,0,0,0-2.43.47A4.78,4.78,0,0,0,3.68,3.68,4.7,4.7,0,0,0,2.53,5.45a7.3,7.3,0,0,0-.47,2.43C2,8.94,2,9.28,2,12s0,3.06.06,4.12a7.3,7.3,0,0,0,.47,2.43,4.7,4.7,0,0,0,1.15,1.77,4.78,4.78,0,0,0,1.77,1.15,7.3,7.3,0,0,0,2.43.47C8.94,22,9.28,22,12,22s3.06,0,4.12-.06a7.3,7.3,0,0,0,2.43-.47,4.7,4.7,0,0,0,1.77-1.15,4.85,4.85,0,0,0,1.16-1.77,7.59,7.59,0,0,0,.46-2.43c0-1.06.06-1.4.06-4.12S22,8.94,21.94,7.88ZM20.14,16a5.61,5.61,0,0,1-.34,1.86,3.06,3.06,0,0,1-.75,1.15,3.19,3.19,0,0,1-1.15.75,5.61,5.61,0,0,1-1.86.34c-1,.05-1.37.06-4,.06s-3,0-4-.06A5.73,5.73,0,0,1,6.1,19.8,3.27,3.27,0,0,1,5,19.05a3,3,0,0,1-.74-1.15A5.54,5.54,0,0,1,3.86,16c0-1-.06-1.37-.06-4s0-3,.06-4A5.54,5.54,0,0,1,4.21,6.1,3,3,0,0,1,5,5,3.14,3.14,0,0,1,6.1,4.2,5.73,5.73,0,0,1,8,3.86c1,0,1.37-.06,4-.06s3,0,4,.06a5.61,5.61,0,0,1,1.86.34A3.06,3.06,0,0,1,19.05,5,3.06,3.06,0,0,1,19.8,6.1,5.61,5.61,0,0,1,20.14,8c.05,1,.06,1.37.06,4S20.19,15,20.14,16ZM12,6.87A5.13,5.13,0,1,0,17.14,12,5.12,5.12,0,0,0,12,6.87Zm0,8.46A3.33,3.33,0,1,1,15.33,12,3.33,3.33,0,0,1,12,15.33Z"/></svg>
</a>

<a id="linkedin-link"
   class="icon" title="Linkedin" aria-label="Linkedin"
   href="https://www.linkedin.com/in/5hyunkwon/"
   target="_blank">
   <svg xmlns="http://www.w3.org/2000/svg" data-name="Layer 1" viewBox="0 0 24 24"><path d="M20.47,2H3.53A1.45,1.45,0,0,0,2.06,3.43V20.57A1.45,1.45,0,0,0,3.53,22H20.47a1.45,1.45,0,0,0,1.47-1.43V3.43A1.45,1.45,0,0,0,20.47,2ZM8.09,18.74h-3v-9h3ZM6.59,8.48h0a1.56,1.56,0,1,1,0-3.12,1.57,1.57,0,1,1,0,3.12ZM18.91,18.74h-3V13.91c0-1.21-.43-2-1.52-2A1.65,1.65,0,0,0,12.85,13a2,2,0,0,0-.1.73v5h-3s0-8.18,0-9h3V11A3,3,0,0,1,15.46,9.5c2,0,3.45,1.29,3.45,4.06Z"/></svg>
</a>
</nav>

  <p>
  &copy; 2021.
  <a href="/LICENSE.md">MIT License.</a>
</p>

</div>

    <main class="container">
      <header>
  <h1 class="post-title">Year Prediction by Movie Posters using CNN - 3</h1>
</header>
<div class="content">
  <div class="post-meta">
  <span class="post-date">29 Jun 2020</span>
  <span class="post-categories">
    
  </span>
</div>


  <div class="post-body">
    <h2 id="2-preprocessing">2. Preprocessing</h2>

<p><code class="highlighter-rouge">Google Colab</code>에서 작업</p>

<h3 id="load-packages">Load Packages</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">layers</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">optimizers</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">keras.callbacks</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span>
<span class="kn">from</span> <span class="nn">keras.applications</span> <span class="kn">import</span> <span class="n">resnet50</span><span class="p">,</span> <span class="n">inception_v3</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tqdm</span><span class="p">().</span><span class="n">pandas</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0it [00:00, ?it/s]
</code></pre></div></div>

<h3 id="load-datasets">Load Datasets</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">total_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'/content/drive/Shared drives/personal/dataframe/total_df.csv'</span><span class="p">)</span>
<span class="n">poster_total</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'/content/drive/Shared drives/personal/poster/poster_total.npy'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">total_df</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>code</th>
      <th>title_kor</th>
      <th>title_eng</th>
      <th>year</th>
      <th>rating</th>
      <th>rank</th>
      <th>link</th>
      <th>genre</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>171539</td>
      <td>그린 북</td>
      <td>Green Book</td>
      <td>2018</td>
      <td>9.59</td>
      <td>12세 관람가</td>
      <td>https://movie.naver.com/movie/bi/mi/basic.nhn?...</td>
      <td>['드라마']</td>
    </tr>
    <tr>
      <th>1</th>
      <td>174830</td>
      <td>가버나움</td>
      <td>Capharnaum</td>
      <td>2018</td>
      <td>9.58</td>
      <td>15세 관람가</td>
      <td>https://movie.naver.com/movie/bi/mi/basic.nhn?...</td>
      <td>['드라마']</td>
    </tr>
    <tr>
      <th>2</th>
      <td>151196</td>
      <td>원더</td>
      <td>Wonder</td>
      <td>2017</td>
      <td>9.49</td>
      <td>전체 관람가</td>
      <td>https://movie.naver.com/movie/bi/mi/basic.nhn?...</td>
      <td>['드라마']</td>
    </tr>
    <tr>
      <th>3</th>
      <td>169240</td>
      <td>아일라</td>
      <td>Ayla: The Daughter of War</td>
      <td>2017</td>
      <td>9.48</td>
      <td>15세 관람가</td>
      <td>https://movie.naver.com/movie/bi/mi/basic.nhn?...</td>
      <td>['드라마', '전쟁']</td>
    </tr>
    <tr>
      <th>4</th>
      <td>157243</td>
      <td>당갈</td>
      <td>Dangal</td>
      <td>2016</td>
      <td>9.47</td>
      <td>12세 관람가</td>
      <td>https://movie.naver.com/movie/bi/mi/basic.nhn?...</td>
      <td>['드라마', '액션']</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1395</th>
      <td>41334</td>
      <td>옹박 - 두번째 미션</td>
      <td>The Protector</td>
      <td>2005</td>
      <td>8.28</td>
      <td>15세 관람가</td>
      <td>https://movie.naver.com/movie/bi/mi/basic.nhn?...</td>
      <td>['액션', '범죄', '드라마', '스릴러']</td>
    </tr>
    <tr>
      <th>1396</th>
      <td>162249</td>
      <td>램페이지</td>
      <td>RAMPAGE</td>
      <td>2018</td>
      <td>8.28</td>
      <td>12세 관람가</td>
      <td>https://movie.naver.com/movie/bi/mi/basic.nhn?...</td>
      <td>['액션', '모험']</td>
    </tr>
    <tr>
      <th>1397</th>
      <td>82473</td>
      <td>캐리비안의 해적: 죽은 자는 말이 없다</td>
      <td>Pirates of the Caribbean: Dead Men Tell No Tales</td>
      <td>2017</td>
      <td>8.28</td>
      <td>12세 관람가</td>
      <td>https://movie.naver.com/movie/bi/mi/basic.nhn?...</td>
      <td>['액션', '모험', '코미디', '판타지']</td>
    </tr>
    <tr>
      <th>1398</th>
      <td>31606</td>
      <td>킬러들의 수다</td>
      <td>Guns &amp; Talks</td>
      <td>2001</td>
      <td>8.28</td>
      <td>15세 관람가</td>
      <td>https://movie.naver.com/movie/bi/mi/basic.nhn?...</td>
      <td>['액션', '드라마', '코미디']</td>
    </tr>
    <tr>
      <th>1399</th>
      <td>51082</td>
      <td>7급 공무원</td>
      <td>7th Grade Civil Servant</td>
      <td>2009</td>
      <td>8.28</td>
      <td>12세 관람가</td>
      <td>https://movie.naver.com/movie/bi/mi/basic.nhn?...</td>
      <td>['액션', '코미디']</td>
    </tr>
  </tbody>
</table>
<p>1400 rows × 8 columns</p>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">poster_total</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[[[  0, 100, 116],
         [  0, 100, 116],
         [  0, 100, 117],
         ...,
         [  1, 115, 141],
         [  0, 116, 141],
         [  1, 117, 142]],

        [[  0, 100, 116],
         [  0, 100, 115],
         [  0, 101, 118],
         ...,
         [  0, 115, 141],
         [  0, 116, 141],
         [  0, 116, 141]],

        [[  0, 101, 116],
         [  0, 101, 116],
         [  0, 101, 118],
         ...,
         [  1, 117, 142],
         [  1, 117, 142],
         [  1, 117, 142]],

        ...,

        [[  0, 106, 130],
         [  1, 107, 131],
         [  0, 108, 132],
         ...,
         [  0, 105, 127],
         [  0, 105, 127],
         [  0, 105, 127]],

        [[  0, 106, 130],
         [  0, 106, 130],
         [  1, 107, 131],
         ...,
         [  1, 104, 127],
         [  1, 104, 126],
         [  1, 104, 127]],

        [[  0, 106, 130],
         [  0, 106, 130],
         [  1, 107, 131],
         ...,
         [  1, 104, 127],
         [  1, 104, 127],
         [  2, 104, 127]]],


       [[[227, 221, 218],
         [231, 231, 227],
         [237, 238, 239],
         ...,
         [ 87,  56,  93],
         [ 89,  58,  94],
         [ 89,  57,  92]],

        [[231, 229, 226],
         [232, 231, 227],
         [237, 237, 236],
         ...,
         [ 89,  58,  95],
         [ 88,  58,  93],
         [ 89,  58,  94]],

        [[236, 235, 233],
         [236, 236, 235],
         [237, 237, 236],
         ...,
         [ 88,  58,  96],
         [ 88,  58,  95],
         [ 89,  58,  94]],

        ...,

        [[104, 129, 151],
         [105, 128, 150],
         [106, 130, 152],
         ...,
         [125, 125,  84],
         [123, 124,  94],
         [ 97, 104, 102]],

        [[103, 129, 151],
         [102, 128, 150],
         [104, 129, 152],
         ...,
         [115, 118,  87],
         [111, 115,  91],
         [ 87,  96,  99]],

        [[100, 129, 149],
         [100, 128, 149],
         [103, 128, 150],
         ...,
         [106, 112,  85],
         [ 97, 106,  86],
         [ 79,  85,  93]]],


       [[[242, 225, 217],
         [241, 223, 213],
         [241, 222, 212],
         ...,
         [250, 250, 231],
         [253, 252, 248],
         [253, 253, 252]],

        [[252, 252, 252],
         [255, 255, 255],
         [255, 255, 255],
         ...,
         [255, 255, 255],
         [255, 255, 255],
         [255, 255, 255]],

        [[247, 234, 229],
         [255, 255, 255],
         [255, 255, 255],
         ...,
         [255, 255, 255],
         [255, 255, 255],
         [255, 255, 255]],

        ...,

        [[100,  97,  86],
         [ 98,  95,  85],
         [ 96,  95,  85],
         ...,
         [114, 102,  86],
         [111,  98,  79],
         [114,  97,  77]],

        [[100,  99,  90],
         [ 89,  88,  79],
         [ 76,  73,  65],
         ...,
         [114, 100,  81],
         [118, 102,  83],
         [119, 100,  80]],

        [[ 90,  89,  83],
         [ 89,  86,  80],
         [ 88,  83,  76],
         ...,
         [102,  90,  73],
         [104,  92,  76],
         [103,  88,  74]]],


       ...,


       [[[107, 147, 155],
         [106, 146, 153],
         [107, 146, 154],
         ...,
         [214, 222, 204],
         [214, 223, 206],
         [213, 222, 204]],

        [[106, 146, 153],
         [106, 147, 154],
         [105, 147, 153],
         ...,
         [215, 222, 204],
         [214, 222, 206],
         [216, 223, 204]],

        [[106, 146, 154],
         [107, 147, 154],
         [104, 146, 152],
         ...,
         [215, 222, 204],
         [215, 222, 205],
         [210, 220, 204]],

        ...,

        [[ 16,  24,  34],
         [ 16,  23,  32],
         [ 16,  23,  32],
         ...,
         [ 16,  21,  28],
         [ 16,  20,  29],
         [ 16,  20,  29]],

        [[ 18,  23,  34],
         [ 17,  23,  34],
         [ 17,  23,  32],
         ...,
         [ 17,  21,  30],
         [ 14,  20,  30],
         [ 15,  20,  30]],

        [[ 18,  22,  33],
         [ 18,  23,  35],
         [ 17,  22,  33],
         ...,
         [ 20,  22,  30],
         [ 45,  29,  30],
         [ 28,  22,  29]]],


       [[[237, 254, 248],
         [250, 250, 250],
         [255, 249, 253],
         ...,
         [252, 250, 250],
         [254, 250, 250],
         [254, 252, 254]],

        [[242, 254, 249],
         [252, 251, 251],
         [244, 243, 245],
         ...,
         [236, 244, 239],
         [253, 253, 253],
         [254, 252, 255]],

        [[248, 253, 251],
         [255, 253, 254],
         [226, 232, 232],
         ...,
         [163, 172, 164],
         [238, 239, 241],
         [254, 253, 255]],

        ...,

        [[252, 252, 252],
         [255, 255, 255],
         [188, 188, 188],
         ...,
         [113, 113, 113],
         [220, 220, 220],
         [255, 255, 255]],

        [[251, 251, 251],
         [251, 251, 251],
         [249, 249, 249],
         ...,
         [152, 152, 152],
         [225, 225, 225],
         [255, 255, 255]],

        [[252, 252, 252],
         [253, 253, 253],
         [254, 254, 254],
         ...,
         [252, 252, 252],
         [252, 252, 252],
         [255, 255, 255]]],


       [[[255, 255, 255],
         [255, 255, 255],
         [255, 255, 255],
         ...,
         [179,  47,  35],
         [179,  47,  35],
         [179,  47,  35]],

        [[255, 255, 255],
         [255, 255, 255],
         [255, 255, 255],
         ...,
         [180,  48,  33],
         [179,  47,  33],
         [179,  47,  33]],

        [[255, 255, 255],
         [255, 255, 255],
         [255, 255, 255],
         ...,
         [181,  49,  32],
         [181,  49,  35],
         [181,  49,  35]],

        ...,

        [[ 96,   4,   8],
         [ 58,   2,   1],
         [ 15,   1,   0],
         ...,
         [136,  27,  33],
         [138,  27,  33],
         [138,  27,  33]],

        [[ 82,   4,   4],
         [ 32,   1,   2],
         [ 12,   2,   3],
         ...,
         [136,  27,  32],
         [138,  27,  33],
         [138,  27,  33]],

        [[ 52,   1,   3],
         [ 10,   1,   2],
         [ 38,   4,   6],
         ...,
         [138,  27,  33],
         [137,  26,  32],
         [138,  27,  33]]]], dtype=uint8)
</code></pre></div></div>

<h3 id="연도-확인하기">연도 확인하기</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">set</span><span class="p">(</span><span class="n">total_df</span><span class="p">[</span><span class="s">'year'</span><span class="p">])</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'...',
 '1936',
 '1939',
 '1940',
 '1942',
 '1953',
 '1954',
 '1959',
 '1960',
 '1965',
 '1968',
 '1972',
 '1973',
 '1974',
 '1975',
 '1976',
 '1978',
 '1979',
 '1980',
 '1981',
 '1983',
 '1984',
 '1985',
 '1986',
 '1987',
 '1988',
 '1989',
 '1990',
 '1991',
 '1992',
 '1993',
 '1994',
 '1995',
 '1996',
 '1997',
 '1998',
 '1999',
 '19세 관람가 도움말',
 '2000',
 '2001',
 '2002',
 '2003',
 '2004',
 '2005',
 '2006',
 '2007',
 '2008',
 '2009',
 '201...',
 '2010',
 '2011',
 '2012',
 '2013',
 '2014',
 '2015',
 '2016',
 '2017',
 '2018',
 '2019',
 '2020'}
</code></pre></div></div>

<p>이상치 제거</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">new_df</span> <span class="o">=</span> <span class="n">total_df</span><span class="p">.</span><span class="n">loc</span><span class="p">[(</span><span class="n">total_df</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span> <span class="o">!=</span> <span class="s">'...'</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">total_df</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span> <span class="o">!=</span> <span class="s">'19세 관람가 도움말'</span><span class="p">)</span> <span class="o">&amp;</span> 
                      <span class="p">(</span><span class="n">total_df</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span> <span class="o">!=</span> <span class="s">'201...'</span><span class="p">)]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">new_df</span><span class="p">[</span><span class="s">'year'</span><span class="p">].</span><span class="n">value_counts</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2007    80
2017    72
2016    72
2006    71
2018    68
2004    67
2015    67
2014    64
2009    63
2012    63
2013    61
2010    60
2008    54
2003    53
2011    52
2019    49
2005    48
2002    36
2001    34
2000    29
1999    24
1994    22
1998    19
1995    18
1993    17
1997    17
1992    16
1996    13
1991     9
1989     8
1988     6
1987     6
1990     6
1986     5
2020     4
1984     3
1972     2
1979     2
1975     2
1981     2
1976     2
1973     2
1985     1
1959     1
1960     1
1980     1
1978     1
1954     1
1942     1
1974     1
1968     1
1940     1
1936     1
1983     1
1953     1
1939     1
1965     1
Name: year, dtype: int64
</code></pre></div></div>

<p>string to int</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">new_df</span><span class="p">[</span><span class="s">'year'</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_df</span><span class="p">[</span><span class="s">'year'</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  """Entry point for launching an IPython kernel.
</code></pre></div></div>

<h3 id="연대로-구분하기">연대로 구분하기</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">era</span><span class="p">(</span><span class="n">year</span><span class="p">):</span>
    <span class="c1"># 2010년대 이후
</span>    <span class="k">if</span> <span class="mi">2010</span> <span class="o">&lt;=</span> <span class="n">year</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="c1"># 2000년대
</span>    <span class="k">elif</span> <span class="mi">2000</span> <span class="o">&lt;=</span> <span class="n">year</span> <span class="o">&lt;</span> <span class="mi">2010</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">1</span>
    <span class="c1"># 2000년대 이전
</span>    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">2</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">new_df</span><span class="p">[</span><span class="s">'era'</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_df</span><span class="p">[</span><span class="s">'year'</span><span class="p">].</span><span class="nb">apply</span><span class="p">(</span><span class="n">era</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  """Entry point for launching an IPython kernel.
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">new_df</span><span class="p">[</span><span class="s">'era'</span><span class="p">].</span><span class="n">value_counts</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0    632
1    535
2    216
Name: era, dtype: int64
</code></pre></div></div>

<p>onehot vector로 만들기</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">to_onehot</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">dimension</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">),</span> <span class="n">dimension</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sequence</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sequences</span><span class="p">):</span>
        <span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">sequence</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span>
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">label_vector</span> <span class="o">=</span> <span class="n">to_onehot</span><span class="p">(</span><span class="n">new_df</span><span class="p">[</span><span class="s">'era'</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">label_vector</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([[1., 0., 0.],
       [1., 0., 0.],
       [1., 0., 0.],
       ...,
       [1., 0., 0.],
       [0., 1., 0.],
       [0., 1., 0.]])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">len</span><span class="p">(</span><span class="n">new_df</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1383
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">new_df</span><span class="p">.</span><span class="n">index</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Int64Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,
            ...
            1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399],
           dtype='int64', length=1383)
</code></pre></div></div>

<h3 id="inputs-scaling">inputs scaling</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">poster_scaled</span> <span class="o">=</span> <span class="n">poster_total</span><span class="p">[</span><span class="n">new_df</span><span class="p">.</span><span class="n">index</span><span class="p">].</span><span class="n">astype</span><span class="p">(</span><span class="s">'float32'</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span>
</code></pre></div></div>

<h2 id="3-modeling">3. Modeling</h2>

<h3 id="train--test-나누기">train / test 나누기</h3>

<p>train 60%, validation 20%, test 20%로 데이터를 나누면<br />
총 1383개의 데이터 중 829개가 train, 277개가 validation, 277개가 test가 됨</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># test 데이터로 20%를 뽑기 위한 랜덤 난수 생성
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">303</span><span class="p">)</span>
<span class="n">shuffled_index</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">permutation</span><span class="p">(</span><span class="mi">1383</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">shuffled_index</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([ 176,  420,  322, ..., 1153,  530,  571])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># train: 0 ~ 1105번 index (1106개)
# test: 1106 ~ 1383번 index (277개)
</span><span class="n">x_train</span> <span class="o">=</span> <span class="n">poster_scaled</span><span class="p">[</span><span class="n">shuffled_index</span><span class="p">[:</span><span class="mi">1106</span><span class="p">]]</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">poster_scaled</span><span class="p">[</span><span class="n">shuffled_index</span><span class="p">[</span><span class="mi">1106</span><span class="p">:]]</span>

<span class="n">y_train</span> <span class="o">=</span> <span class="n">label_vector</span><span class="p">[</span><span class="n">shuffled_index</span><span class="p">[:</span><span class="mi">1106</span><span class="p">]]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">label_vector</span><span class="p">[</span><span class="n">shuffled_index</span><span class="p">[</span><span class="mi">1106</span><span class="p">:]]</span>
</code></pre></div></div>

<p>train data shuffling</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">8989</span><span class="p">)</span>
<span class="n">s_index</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">permutation</span><span class="p">(</span><span class="mi">1106</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">s_index</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">s_index</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_train</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(1106, 256, 256, 3)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_train</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(1106, 3)
</code></pre></div></div>

<h3 id="a-basic-cnn">a. Basic CNN</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">build_model</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">,</span>
                            <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
    <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">))</span>
    <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
    <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">))</span>
    <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">))</span>
    <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
    <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">))</span>
    <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">))</span>
    <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
    <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">())</span>
    <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
    <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">))</span>
    <span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizers</span><span class="p">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">),</span>
                  <span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span>
                  <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="s">'default_model'</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "default_model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 256, 256, 32)      896       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 256, 256, 32)      9248      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 128, 128, 32)      0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 128, 128, 64)      18496     
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 128, 128, 64)      36928     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 64, 64, 64)        0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 64, 64, 128)       73856     
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 64, 64, 128)       147584    
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 32, 32, 128)       0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 131072)            0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               67109376  
_________________________________________________________________
dense_2 (Dense)              (None, 3)                 1539      
=================================================================
Total params: 67,397,923
Trainable params: 67,397,923
Non-trainable params: 0
_________________________________________________________________
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">model_train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">):</span> 
    <span class="n">filepath</span> <span class="o">=</span> <span class="s">'/content/drive/Shared drives/personal/weights/'</span> <span class="o">+</span> <span class="n">model</span><span class="p">.</span><span class="n">name</span> <span class="o">+</span> <span class="s">'.{epoch:02d}-{val_accuracy:.4f}.hdf5'</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s">'val_accuracy'</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">save_weights_only</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">save_best_only</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">'auto'</span><span class="p">,</span> <span class="n">period</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">model_history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpoint</span><span class="p">],</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">.</span><span class="n">history</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">history</span> <span class="o">=</span> <span class="n">model_train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Train on 829 samples, validate on 277 samples
Epoch 1/20
829/829 [==============================] - 11s 13ms/step - loss: 3.4242 - accuracy: 0.4282 - val_loss: 1.0881 - val_accuracy: 0.3574
Epoch 2/20
829/829 [==============================] - 4s 4ms/step - loss: 1.0365 - accuracy: 0.4632 - val_loss: 1.0050 - val_accuracy: 0.4874
Epoch 3/20
829/829 [==============================] - 4s 4ms/step - loss: 1.0529 - accuracy: 0.4415 - val_loss: 1.0087 - val_accuracy: 0.4765
Epoch 4/20
829/829 [==============================] - 4s 4ms/step - loss: 1.1031 - accuracy: 0.4572 - val_loss: 1.0188 - val_accuracy: 0.4549
Epoch 5/20
829/829 [==============================] - 4s 4ms/step - loss: 1.0396 - accuracy: 0.5139 - val_loss: 1.1308 - val_accuracy: 0.4910
Epoch 6/20
829/829 [==============================] - 4s 4ms/step - loss: 0.9136 - accuracy: 0.5875 - val_loss: 1.0257 - val_accuracy: 0.5090
Epoch 7/20
829/829 [==============================] - 4s 4ms/step - loss: 0.7788 - accuracy: 0.6381 - val_loss: 1.0869 - val_accuracy: 0.4188
Epoch 8/20
829/829 [==============================] - 4s 4ms/step - loss: 0.6363 - accuracy: 0.7419 - val_loss: 1.2535 - val_accuracy: 0.4477
Epoch 9/20
829/829 [==============================] - 4s 4ms/step - loss: 0.3825 - accuracy: 0.8480 - val_loss: 2.6986 - val_accuracy: 0.4296
Epoch 10/20
829/829 [==============================] - 4s 4ms/step - loss: 0.2134 - accuracy: 0.9192 - val_loss: 3.7182 - val_accuracy: 0.4332
Epoch 11/20
829/829 [==============================] - 4s 4ms/step - loss: 0.1467 - accuracy: 0.9554 - val_loss: 3.2280 - val_accuracy: 0.4440
Epoch 12/20
829/829 [==============================] - 4s 4ms/step - loss: 0.1742 - accuracy: 0.9831 - val_loss: 5.8329 - val_accuracy: 0.4585
Epoch 13/20
829/829 [==============================] - 4s 4ms/step - loss: 0.1002 - accuracy: 0.9843 - val_loss: 5.0345 - val_accuracy: 0.4874
Epoch 14/20
829/829 [==============================] - 4s 4ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 8.9527 - val_accuracy: 0.4440
Epoch 15/20
829/829 [==============================] - 4s 4ms/step - loss: 2.7524e-05 - accuracy: 1.0000 - val_loss: 11.7579 - val_accuracy: 0.4910
Epoch 16/20
829/829 [==============================] - 4s 4ms/step - loss: 2.6138e-06 - accuracy: 1.0000 - val_loss: 14.4503 - val_accuracy: 0.4729
Epoch 17/20
829/829 [==============================] - 4s 4ms/step - loss: 1.7256e-08 - accuracy: 1.0000 - val_loss: 14.9324 - val_accuracy: 0.4874
Epoch 18/20
829/829 [==============================] - 4s 4ms/step - loss: 6.1833e-09 - accuracy: 1.0000 - val_loss: 16.6347 - val_accuracy: 0.4657
Epoch 19/20
829/829 [==============================] - 4s 4ms/step - loss: 2.8760e-10 - accuracy: 1.0000 - val_loss: 16.6152 - val_accuracy: 0.4693
Epoch 20/20
829/829 [==============================] - 4s 4ms/step - loss: 1.4380e-10 - accuracy: 1.0000 - val_loss: 16.5649 - val_accuracy: 0.4621
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">history_dict</span> <span class="o">=</span> <span class="n">history</span><span class="p">.</span><span class="n">history</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s">'loss'</span><span class="p">]</span>
<span class="n">val_loss</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">]</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s">'bo'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Validation loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Training and validation loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Epochs'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/img/movie_posters/output_48_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">acc</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">]</span>
<span class="n">val_acc</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s">'val_accuracy'</span><span class="p">]</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="s">'bo'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training acc'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Validation acc'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Training and validation acc'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Epochs'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/img/movie_posters/output_49_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s">'/content/drive/Shared drives/personal/weights/default_model.06-0.5090.hdf5'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>277/277 [==============================] - 1s 3ms/step





[0.9569806161770321, 0.505415141582489]
</code></pre></div></div>

<h3 id="b-resnet50">b. ResNet50</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">build_resnet</span><span class="p">():</span>
    <span class="c1"># keras 기본 모델 이용
</span>    <span class="n">base_model</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">.</span><span class="n">ResNet50</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">include_top</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span> <span class="p">,</span><span class="mi">3</span><span class="p">))</span>
    <span class="n">output_model</span> <span class="o">=</span> <span class="n">base_model</span><span class="p">.</span><span class="n">output</span>
    <span class="n">flatten_model</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">output_model</span><span class="p">)</span>
    <span class="n">dense_model</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">flatten_model</span><span class="p">)</span>
    <span class="n">final_model</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">)(</span><span class="n">dense_model</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">base_model</span><span class="p">.</span><span class="nb">input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">final_model</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizers</span><span class="p">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">),</span>
                <span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span>
                <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">build_resnet</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/usr/local/lib/python3.6/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.
  warnings.warn('The output shape of `ResNet50(include_top=False)` '
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="s">'resnet_model'</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "resnet_model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_8 (InputLayer)            (None, 256, 256, 3)  0                                            
__________________________________________________________________________________________________
conv1_pad (ZeroPadding2D)       (None, 262, 262, 3)  0           input_8[0][0]                    
__________________________________________________________________________________________________
conv1 (Conv2D)                  (None, 128, 128, 64) 9472        conv1_pad[0][0]                  
__________________________________________________________________________________________________
bn_conv1 (BatchNormalization)   (None, 128, 128, 64) 256         conv1[0][0]                      
__________________________________________________________________________________________________
activation_246 (Activation)     (None, 128, 128, 64) 0           bn_conv1[0][0]                   
__________________________________________________________________________________________________
pool1_pad (ZeroPadding2D)       (None, 130, 130, 64) 0           activation_246[0][0]             
__________________________________________________________________________________________________
max_pooling2d_17 (MaxPooling2D) (None, 64, 64, 64)   0           pool1_pad[0][0]                  
__________________________________________________________________________________________________
res2a_branch2a (Conv2D)         (None, 64, 64, 64)   4160        max_pooling2d_17[0][0]           
__________________________________________________________________________________________________
bn2a_branch2a (BatchNormalizati (None, 64, 64, 64)   256         res2a_branch2a[0][0]             
__________________________________________________________________________________________________
activation_247 (Activation)     (None, 64, 64, 64)   0           bn2a_branch2a[0][0]              
__________________________________________________________________________________________________
res2a_branch2b (Conv2D)         (None, 64, 64, 64)   36928       activation_247[0][0]             
__________________________________________________________________________________________________
bn2a_branch2b (BatchNormalizati (None, 64, 64, 64)   256         res2a_branch2b[0][0]             
__________________________________________________________________________________________________
activation_248 (Activation)     (None, 64, 64, 64)   0           bn2a_branch2b[0][0]              
__________________________________________________________________________________________________
res2a_branch2c (Conv2D)         (None, 64, 64, 256)  16640       activation_248[0][0]             
__________________________________________________________________________________________________
res2a_branch1 (Conv2D)          (None, 64, 64, 256)  16640       max_pooling2d_17[0][0]           
__________________________________________________________________________________________________
bn2a_branch2c (BatchNormalizati (None, 64, 64, 256)  1024        res2a_branch2c[0][0]             
__________________________________________________________________________________________________
bn2a_branch1 (BatchNormalizatio (None, 64, 64, 256)  1024        res2a_branch1[0][0]              
__________________________________________________________________________________________________
add_81 (Add)                    (None, 64, 64, 256)  0           bn2a_branch2c[0][0]              
                                                                 bn2a_branch1[0][0]               
__________________________________________________________________________________________________
activation_249 (Activation)     (None, 64, 64, 256)  0           add_81[0][0]                     
__________________________________________________________________________________________________
res2b_branch2a (Conv2D)         (None, 64, 64, 64)   16448       activation_249[0][0]             
__________________________________________________________________________________________________
bn2b_branch2a (BatchNormalizati (None, 64, 64, 64)   256         res2b_branch2a[0][0]             
__________________________________________________________________________________________________
activation_250 (Activation)     (None, 64, 64, 64)   0           bn2b_branch2a[0][0]              
__________________________________________________________________________________________________
res2b_branch2b (Conv2D)         (None, 64, 64, 64)   36928       activation_250[0][0]             
__________________________________________________________________________________________________
bn2b_branch2b (BatchNormalizati (None, 64, 64, 64)   256         res2b_branch2b[0][0]             
__________________________________________________________________________________________________
activation_251 (Activation)     (None, 64, 64, 64)   0           bn2b_branch2b[0][0]              
__________________________________________________________________________________________________
res2b_branch2c (Conv2D)         (None, 64, 64, 256)  16640       activation_251[0][0]             
__________________________________________________________________________________________________
bn2b_branch2c (BatchNormalizati (None, 64, 64, 256)  1024        res2b_branch2c[0][0]             
__________________________________________________________________________________________________
add_82 (Add)                    (None, 64, 64, 256)  0           bn2b_branch2c[0][0]              
                                                                 activation_249[0][0]             
__________________________________________________________________________________________________
activation_252 (Activation)     (None, 64, 64, 256)  0           add_82[0][0]                     
__________________________________________________________________________________________________
res2c_branch2a (Conv2D)         (None, 64, 64, 64)   16448       activation_252[0][0]             
__________________________________________________________________________________________________
bn2c_branch2a (BatchNormalizati (None, 64, 64, 64)   256         res2c_branch2a[0][0]             
__________________________________________________________________________________________________
activation_253 (Activation)     (None, 64, 64, 64)   0           bn2c_branch2a[0][0]              
__________________________________________________________________________________________________
res2c_branch2b (Conv2D)         (None, 64, 64, 64)   36928       activation_253[0][0]             
__________________________________________________________________________________________________
bn2c_branch2b (BatchNormalizati (None, 64, 64, 64)   256         res2c_branch2b[0][0]             
__________________________________________________________________________________________________
activation_254 (Activation)     (None, 64, 64, 64)   0           bn2c_branch2b[0][0]              
__________________________________________________________________________________________________
res2c_branch2c (Conv2D)         (None, 64, 64, 256)  16640       activation_254[0][0]             
__________________________________________________________________________________________________
bn2c_branch2c (BatchNormalizati (None, 64, 64, 256)  1024        res2c_branch2c[0][0]             
__________________________________________________________________________________________________
add_83 (Add)                    (None, 64, 64, 256)  0           bn2c_branch2c[0][0]              
                                                                 activation_252[0][0]             
__________________________________________________________________________________________________
activation_255 (Activation)     (None, 64, 64, 256)  0           add_83[0][0]                     
__________________________________________________________________________________________________
res3a_branch2a (Conv2D)         (None, 32, 32, 128)  32896       activation_255[0][0]             
__________________________________________________________________________________________________
bn3a_branch2a (BatchNormalizati (None, 32, 32, 128)  512         res3a_branch2a[0][0]             
__________________________________________________________________________________________________
activation_256 (Activation)     (None, 32, 32, 128)  0           bn3a_branch2a[0][0]              
__________________________________________________________________________________________________
res3a_branch2b (Conv2D)         (None, 32, 32, 128)  147584      activation_256[0][0]             
__________________________________________________________________________________________________
bn3a_branch2b (BatchNormalizati (None, 32, 32, 128)  512         res3a_branch2b[0][0]             
__________________________________________________________________________________________________
activation_257 (Activation)     (None, 32, 32, 128)  0           bn3a_branch2b[0][0]              
__________________________________________________________________________________________________
res3a_branch2c (Conv2D)         (None, 32, 32, 512)  66048       activation_257[0][0]             
__________________________________________________________________________________________________
res3a_branch1 (Conv2D)          (None, 32, 32, 512)  131584      activation_255[0][0]             
__________________________________________________________________________________________________
bn3a_branch2c (BatchNormalizati (None, 32, 32, 512)  2048        res3a_branch2c[0][0]             
__________________________________________________________________________________________________
bn3a_branch1 (BatchNormalizatio (None, 32, 32, 512)  2048        res3a_branch1[0][0]              
__________________________________________________________________________________________________
add_84 (Add)                    (None, 32, 32, 512)  0           bn3a_branch2c[0][0]              
                                                                 bn3a_branch1[0][0]               
__________________________________________________________________________________________________
activation_258 (Activation)     (None, 32, 32, 512)  0           add_84[0][0]                     
__________________________________________________________________________________________________
res3b_branch2a (Conv2D)         (None, 32, 32, 128)  65664       activation_258[0][0]             
__________________________________________________________________________________________________
bn3b_branch2a (BatchNormalizati (None, 32, 32, 128)  512         res3b_branch2a[0][0]             
__________________________________________________________________________________________________
activation_259 (Activation)     (None, 32, 32, 128)  0           bn3b_branch2a[0][0]              
__________________________________________________________________________________________________
res3b_branch2b (Conv2D)         (None, 32, 32, 128)  147584      activation_259[0][0]             
__________________________________________________________________________________________________
bn3b_branch2b (BatchNormalizati (None, 32, 32, 128)  512         res3b_branch2b[0][0]             
__________________________________________________________________________________________________
activation_260 (Activation)     (None, 32, 32, 128)  0           bn3b_branch2b[0][0]              
__________________________________________________________________________________________________
res3b_branch2c (Conv2D)         (None, 32, 32, 512)  66048       activation_260[0][0]             
__________________________________________________________________________________________________
bn3b_branch2c (BatchNormalizati (None, 32, 32, 512)  2048        res3b_branch2c[0][0]             
__________________________________________________________________________________________________
add_85 (Add)                    (None, 32, 32, 512)  0           bn3b_branch2c[0][0]              
                                                                 activation_258[0][0]             
__________________________________________________________________________________________________
activation_261 (Activation)     (None, 32, 32, 512)  0           add_85[0][0]                     
__________________________________________________________________________________________________
res3c_branch2a (Conv2D)         (None, 32, 32, 128)  65664       activation_261[0][0]             
__________________________________________________________________________________________________
bn3c_branch2a (BatchNormalizati (None, 32, 32, 128)  512         res3c_branch2a[0][0]             
__________________________________________________________________________________________________
activation_262 (Activation)     (None, 32, 32, 128)  0           bn3c_branch2a[0][0]              
__________________________________________________________________________________________________
res3c_branch2b (Conv2D)         (None, 32, 32, 128)  147584      activation_262[0][0]             
__________________________________________________________________________________________________
bn3c_branch2b (BatchNormalizati (None, 32, 32, 128)  512         res3c_branch2b[0][0]             
__________________________________________________________________________________________________
activation_263 (Activation)     (None, 32, 32, 128)  0           bn3c_branch2b[0][0]              
__________________________________________________________________________________________________
res3c_branch2c (Conv2D)         (None, 32, 32, 512)  66048       activation_263[0][0]             
__________________________________________________________________________________________________
bn3c_branch2c (BatchNormalizati (None, 32, 32, 512)  2048        res3c_branch2c[0][0]             
__________________________________________________________________________________________________
add_86 (Add)                    (None, 32, 32, 512)  0           bn3c_branch2c[0][0]              
                                                                 activation_261[0][0]             
__________________________________________________________________________________________________
activation_264 (Activation)     (None, 32, 32, 512)  0           add_86[0][0]                     
__________________________________________________________________________________________________
res3d_branch2a (Conv2D)         (None, 32, 32, 128)  65664       activation_264[0][0]             
__________________________________________________________________________________________________
bn3d_branch2a (BatchNormalizati (None, 32, 32, 128)  512         res3d_branch2a[0][0]             
__________________________________________________________________________________________________
activation_265 (Activation)     (None, 32, 32, 128)  0           bn3d_branch2a[0][0]              
__________________________________________________________________________________________________
res3d_branch2b (Conv2D)         (None, 32, 32, 128)  147584      activation_265[0][0]             
__________________________________________________________________________________________________
bn3d_branch2b (BatchNormalizati (None, 32, 32, 128)  512         res3d_branch2b[0][0]             
__________________________________________________________________________________________________
activation_266 (Activation)     (None, 32, 32, 128)  0           bn3d_branch2b[0][0]              
__________________________________________________________________________________________________
res3d_branch2c (Conv2D)         (None, 32, 32, 512)  66048       activation_266[0][0]             
__________________________________________________________________________________________________
bn3d_branch2c (BatchNormalizati (None, 32, 32, 512)  2048        res3d_branch2c[0][0]             
__________________________________________________________________________________________________
add_87 (Add)                    (None, 32, 32, 512)  0           bn3d_branch2c[0][0]              
                                                                 activation_264[0][0]             
__________________________________________________________________________________________________
activation_267 (Activation)     (None, 32, 32, 512)  0           add_87[0][0]                     
__________________________________________________________________________________________________
res4a_branch2a (Conv2D)         (None, 16, 16, 256)  131328      activation_267[0][0]             
__________________________________________________________________________________________________
bn4a_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4a_branch2a[0][0]             
__________________________________________________________________________________________________
activation_268 (Activation)     (None, 16, 16, 256)  0           bn4a_branch2a[0][0]              
__________________________________________________________________________________________________
res4a_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_268[0][0]             
__________________________________________________________________________________________________
bn4a_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4a_branch2b[0][0]             
__________________________________________________________________________________________________
activation_269 (Activation)     (None, 16, 16, 256)  0           bn4a_branch2b[0][0]              
__________________________________________________________________________________________________
res4a_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_269[0][0]             
__________________________________________________________________________________________________
res4a_branch1 (Conv2D)          (None, 16, 16, 1024) 525312      activation_267[0][0]             
__________________________________________________________________________________________________
bn4a_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4a_branch2c[0][0]             
__________________________________________________________________________________________________
bn4a_branch1 (BatchNormalizatio (None, 16, 16, 1024) 4096        res4a_branch1[0][0]              
__________________________________________________________________________________________________
add_88 (Add)                    (None, 16, 16, 1024) 0           bn4a_branch2c[0][0]              
                                                                 bn4a_branch1[0][0]               
__________________________________________________________________________________________________
activation_270 (Activation)     (None, 16, 16, 1024) 0           add_88[0][0]                     
__________________________________________________________________________________________________
res4b_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation_270[0][0]             
__________________________________________________________________________________________________
bn4b_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4b_branch2a[0][0]             
__________________________________________________________________________________________________
activation_271 (Activation)     (None, 16, 16, 256)  0           bn4b_branch2a[0][0]              
__________________________________________________________________________________________________
res4b_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_271[0][0]             
__________________________________________________________________________________________________
bn4b_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4b_branch2b[0][0]             
__________________________________________________________________________________________________
activation_272 (Activation)     (None, 16, 16, 256)  0           bn4b_branch2b[0][0]              
__________________________________________________________________________________________________
res4b_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_272[0][0]             
__________________________________________________________________________________________________
bn4b_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4b_branch2c[0][0]             
__________________________________________________________________________________________________
add_89 (Add)                    (None, 16, 16, 1024) 0           bn4b_branch2c[0][0]              
                                                                 activation_270[0][0]             
__________________________________________________________________________________________________
activation_273 (Activation)     (None, 16, 16, 1024) 0           add_89[0][0]                     
__________________________________________________________________________________________________
res4c_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation_273[0][0]             
__________________________________________________________________________________________________
bn4c_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4c_branch2a[0][0]             
__________________________________________________________________________________________________
activation_274 (Activation)     (None, 16, 16, 256)  0           bn4c_branch2a[0][0]              
__________________________________________________________________________________________________
res4c_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_274[0][0]             
__________________________________________________________________________________________________
bn4c_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4c_branch2b[0][0]             
__________________________________________________________________________________________________
activation_275 (Activation)     (None, 16, 16, 256)  0           bn4c_branch2b[0][0]              
__________________________________________________________________________________________________
res4c_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_275[0][0]             
__________________________________________________________________________________________________
bn4c_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4c_branch2c[0][0]             
__________________________________________________________________________________________________
add_90 (Add)                    (None, 16, 16, 1024) 0           bn4c_branch2c[0][0]              
                                                                 activation_273[0][0]             
__________________________________________________________________________________________________
activation_276 (Activation)     (None, 16, 16, 1024) 0           add_90[0][0]                     
__________________________________________________________________________________________________
res4d_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation_276[0][0]             
__________________________________________________________________________________________________
bn4d_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4d_branch2a[0][0]             
__________________________________________________________________________________________________
activation_277 (Activation)     (None, 16, 16, 256)  0           bn4d_branch2a[0][0]              
__________________________________________________________________________________________________
res4d_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_277[0][0]             
__________________________________________________________________________________________________
bn4d_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4d_branch2b[0][0]             
__________________________________________________________________________________________________
activation_278 (Activation)     (None, 16, 16, 256)  0           bn4d_branch2b[0][0]              
__________________________________________________________________________________________________
res4d_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_278[0][0]             
__________________________________________________________________________________________________
bn4d_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4d_branch2c[0][0]             
__________________________________________________________________________________________________
add_91 (Add)                    (None, 16, 16, 1024) 0           bn4d_branch2c[0][0]              
                                                                 activation_276[0][0]             
__________________________________________________________________________________________________
activation_279 (Activation)     (None, 16, 16, 1024) 0           add_91[0][0]                     
__________________________________________________________________________________________________
res4e_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation_279[0][0]             
__________________________________________________________________________________________________
bn4e_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4e_branch2a[0][0]             
__________________________________________________________________________________________________
activation_280 (Activation)     (None, 16, 16, 256)  0           bn4e_branch2a[0][0]              
__________________________________________________________________________________________________
res4e_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_280[0][0]             
__________________________________________________________________________________________________
bn4e_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4e_branch2b[0][0]             
__________________________________________________________________________________________________
activation_281 (Activation)     (None, 16, 16, 256)  0           bn4e_branch2b[0][0]              
__________________________________________________________________________________________________
res4e_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_281[0][0]             
__________________________________________________________________________________________________
bn4e_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4e_branch2c[0][0]             
__________________________________________________________________________________________________
add_92 (Add)                    (None, 16, 16, 1024) 0           bn4e_branch2c[0][0]              
                                                                 activation_279[0][0]             
__________________________________________________________________________________________________
activation_282 (Activation)     (None, 16, 16, 1024) 0           add_92[0][0]                     
__________________________________________________________________________________________________
res4f_branch2a (Conv2D)         (None, 16, 16, 256)  262400      activation_282[0][0]             
__________________________________________________________________________________________________
bn4f_branch2a (BatchNormalizati (None, 16, 16, 256)  1024        res4f_branch2a[0][0]             
__________________________________________________________________________________________________
activation_283 (Activation)     (None, 16, 16, 256)  0           bn4f_branch2a[0][0]              
__________________________________________________________________________________________________
res4f_branch2b (Conv2D)         (None, 16, 16, 256)  590080      activation_283[0][0]             
__________________________________________________________________________________________________
bn4f_branch2b (BatchNormalizati (None, 16, 16, 256)  1024        res4f_branch2b[0][0]             
__________________________________________________________________________________________________
activation_284 (Activation)     (None, 16, 16, 256)  0           bn4f_branch2b[0][0]              
__________________________________________________________________________________________________
res4f_branch2c (Conv2D)         (None, 16, 16, 1024) 263168      activation_284[0][0]             
__________________________________________________________________________________________________
bn4f_branch2c (BatchNormalizati (None, 16, 16, 1024) 4096        res4f_branch2c[0][0]             
__________________________________________________________________________________________________
add_93 (Add)                    (None, 16, 16, 1024) 0           bn4f_branch2c[0][0]              
                                                                 activation_282[0][0]             
__________________________________________________________________________________________________
activation_285 (Activation)     (None, 16, 16, 1024) 0           add_93[0][0]                     
__________________________________________________________________________________________________
res5a_branch2a (Conv2D)         (None, 8, 8, 512)    524800      activation_285[0][0]             
__________________________________________________________________________________________________
bn5a_branch2a (BatchNormalizati (None, 8, 8, 512)    2048        res5a_branch2a[0][0]             
__________________________________________________________________________________________________
activation_286 (Activation)     (None, 8, 8, 512)    0           bn5a_branch2a[0][0]              
__________________________________________________________________________________________________
res5a_branch2b (Conv2D)         (None, 8, 8, 512)    2359808     activation_286[0][0]             
__________________________________________________________________________________________________
bn5a_branch2b (BatchNormalizati (None, 8, 8, 512)    2048        res5a_branch2b[0][0]             
__________________________________________________________________________________________________
activation_287 (Activation)     (None, 8, 8, 512)    0           bn5a_branch2b[0][0]              
__________________________________________________________________________________________________
res5a_branch2c (Conv2D)         (None, 8, 8, 2048)   1050624     activation_287[0][0]             
__________________________________________________________________________________________________
res5a_branch1 (Conv2D)          (None, 8, 8, 2048)   2099200     activation_285[0][0]             
__________________________________________________________________________________________________
bn5a_branch2c (BatchNormalizati (None, 8, 8, 2048)   8192        res5a_branch2c[0][0]             
__________________________________________________________________________________________________
bn5a_branch1 (BatchNormalizatio (None, 8, 8, 2048)   8192        res5a_branch1[0][0]              
__________________________________________________________________________________________________
add_94 (Add)                    (None, 8, 8, 2048)   0           bn5a_branch2c[0][0]              
                                                                 bn5a_branch1[0][0]               
__________________________________________________________________________________________________
activation_288 (Activation)     (None, 8, 8, 2048)   0           add_94[0][0]                     
__________________________________________________________________________________________________
res5b_branch2a (Conv2D)         (None, 8, 8, 512)    1049088     activation_288[0][0]             
__________________________________________________________________________________________________
bn5b_branch2a (BatchNormalizati (None, 8, 8, 512)    2048        res5b_branch2a[0][0]             
__________________________________________________________________________________________________
activation_289 (Activation)     (None, 8, 8, 512)    0           bn5b_branch2a[0][0]              
__________________________________________________________________________________________________
res5b_branch2b (Conv2D)         (None, 8, 8, 512)    2359808     activation_289[0][0]             
__________________________________________________________________________________________________
bn5b_branch2b (BatchNormalizati (None, 8, 8, 512)    2048        res5b_branch2b[0][0]             
__________________________________________________________________________________________________
activation_290 (Activation)     (None, 8, 8, 512)    0           bn5b_branch2b[0][0]              
__________________________________________________________________________________________________
res5b_branch2c (Conv2D)         (None, 8, 8, 2048)   1050624     activation_290[0][0]             
__________________________________________________________________________________________________
bn5b_branch2c (BatchNormalizati (None, 8, 8, 2048)   8192        res5b_branch2c[0][0]             
__________________________________________________________________________________________________
add_95 (Add)                    (None, 8, 8, 2048)   0           bn5b_branch2c[0][0]              
                                                                 activation_288[0][0]             
__________________________________________________________________________________________________
activation_291 (Activation)     (None, 8, 8, 2048)   0           add_95[0][0]                     
__________________________________________________________________________________________________
res5c_branch2a (Conv2D)         (None, 8, 8, 512)    1049088     activation_291[0][0]             
__________________________________________________________________________________________________
bn5c_branch2a (BatchNormalizati (None, 8, 8, 512)    2048        res5c_branch2a[0][0]             
__________________________________________________________________________________________________
activation_292 (Activation)     (None, 8, 8, 512)    0           bn5c_branch2a[0][0]              
__________________________________________________________________________________________________
res5c_branch2b (Conv2D)         (None, 8, 8, 512)    2359808     activation_292[0][0]             
__________________________________________________________________________________________________
bn5c_branch2b (BatchNormalizati (None, 8, 8, 512)    2048        res5c_branch2b[0][0]             
__________________________________________________________________________________________________
activation_293 (Activation)     (None, 8, 8, 512)    0           bn5c_branch2b[0][0]              
__________________________________________________________________________________________________
res5c_branch2c (Conv2D)         (None, 8, 8, 2048)   1050624     activation_293[0][0]             
__________________________________________________________________________________________________
bn5c_branch2c (BatchNormalizati (None, 8, 8, 2048)   8192        res5c_branch2c[0][0]             
__________________________________________________________________________________________________
add_96 (Add)                    (None, 8, 8, 2048)   0           bn5c_branch2c[0][0]              
                                                                 activation_291[0][0]             
__________________________________________________________________________________________________
activation_294 (Activation)     (None, 8, 8, 2048)   0           add_96[0][0]                     
__________________________________________________________________________________________________
flatten_5 (Flatten)             (None, 131072)       0           activation_294[0][0]             
__________________________________________________________________________________________________
dense_12 (Dense)                (None, 512)          67109376    flatten_5[0][0]                  
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 3)            1539        dense_12[0][0]                   
==================================================================================================
Total params: 90,698,627
Trainable params: 90,645,507
Non-trainable params: 53,120
__________________________________________________________________________________________________
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">history</span> <span class="o">=</span> <span class="n">model_train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="mi">40</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Train on 829 samples, validate on 277 samples
Epoch 1/40
829/829 [==============================] - 27s 32ms/step - loss: 110.0953 - accuracy: 0.3390 - val_loss: 1355.0505 - val_accuracy: 0.3069
Epoch 2/40
829/829 [==============================] - 15s 18ms/step - loss: 4.3456 - accuracy: 0.4089 - val_loss: 56807.1282 - val_accuracy: 0.3430
Epoch 3/40
829/829 [==============================] - 14s 17ms/step - loss: 1.9580 - accuracy: 0.4101 - val_loss: 776.6751 - val_accuracy: 0.4043
Epoch 4/40
829/829 [==============================] - 14s 17ms/step - loss: 1.2369 - accuracy: 0.4234 - val_loss: 2.1374 - val_accuracy: 0.4513
Epoch 5/40
829/829 [==============================] - 15s 18ms/step - loss: 1.0994 - accuracy: 0.4017 - val_loss: 6.5897 - val_accuracy: 0.2996
Epoch 6/40
829/829 [==============================] - 14s 17ms/step - loss: 1.0461 - accuracy: 0.4343 - val_loss: 40.6376 - val_accuracy: 0.3682
Epoch 7/40
829/829 [==============================] - 14s 17ms/step - loss: 1.0381 - accuracy: 0.4234 - val_loss: 83.7707 - val_accuracy: 0.3827
Epoch 8/40
829/829 [==============================] - 14s 17ms/step - loss: 1.1470 - accuracy: 0.4234 - val_loss: 1.0714 - val_accuracy: 0.4910
Epoch 9/40
829/829 [==============================] - 14s 17ms/step - loss: 1.0752 - accuracy: 0.4524 - val_loss: 1.1472 - val_accuracy: 0.4621
Epoch 10/40
829/829 [==============================] - 14s 17ms/step - loss: 1.0367 - accuracy: 0.4680 - val_loss: 4.5824 - val_accuracy: 0.3574
Epoch 11/40
829/829 [==============================] - 14s 17ms/step - loss: 1.0282 - accuracy: 0.4379 - val_loss: 48.4230 - val_accuracy: 0.4838
Epoch 12/40
829/829 [==============================] - 14s 17ms/step - loss: 1.0280 - accuracy: 0.4258 - val_loss: 2.9461 - val_accuracy: 0.4910
Epoch 13/40
829/829 [==============================] - 14s 17ms/step - loss: 1.0164 - accuracy: 0.4258 - val_loss: 14.7316 - val_accuracy: 0.4765
Epoch 14/40
829/829 [==============================] - 14s 17ms/step - loss: 1.0365 - accuracy: 0.4270 - val_loss: 30.3243 - val_accuracy: 0.4874
Epoch 15/40
829/829 [==============================] - 14s 17ms/step - loss: 1.0087 - accuracy: 0.4210 - val_loss: 2.2417 - val_accuracy: 0.4693
Epoch 16/40
829/829 [==============================] - 14s 17ms/step - loss: 0.9984 - accuracy: 0.4246 - val_loss: 3.3897 - val_accuracy: 0.3755
Epoch 17/40
829/829 [==============================] - 14s 17ms/step - loss: 0.9947 - accuracy: 0.4210 - val_loss: 2.5354 - val_accuracy: 0.4332
Epoch 18/40
829/829 [==============================] - 15s 18ms/step - loss: 0.9816 - accuracy: 0.4294 - val_loss: 2.5836 - val_accuracy: 0.4260
Epoch 19/40
829/829 [==============================] - 15s 18ms/step - loss: 0.9905 - accuracy: 0.4367 - val_loss: 3.7856 - val_accuracy: 0.4946
Epoch 20/40
829/829 [==============================] - 15s 18ms/step - loss: 0.9871 - accuracy: 0.4367 - val_loss: 5.0321 - val_accuracy: 0.3610
Epoch 21/40
829/829 [==============================] - 15s 17ms/step - loss: 0.9815 - accuracy: 0.4511 - val_loss: 1.1294 - val_accuracy: 0.4838
Epoch 22/40
829/829 [==============================] - 15s 17ms/step - loss: 0.9706 - accuracy: 0.4499 - val_loss: 1.0897 - val_accuracy: 0.5126
Epoch 23/40
829/829 [==============================] - 15s 17ms/step - loss: 0.9557 - accuracy: 0.4560 - val_loss: 1.1240 - val_accuracy: 0.5054
Epoch 24/40
829/829 [==============================] - 14s 17ms/step - loss: 0.9380 - accuracy: 0.4511 - val_loss: 2.8845 - val_accuracy: 0.3755
Epoch 25/40
829/829 [==============================] - 15s 18ms/step - loss: 0.9309 - accuracy: 0.4596 - val_loss: 1.5849 - val_accuracy: 0.4765
Epoch 26/40
829/829 [==============================] - 15s 18ms/step - loss: 0.9354 - accuracy: 0.4885 - val_loss: 1.0172 - val_accuracy: 0.5126
Epoch 27/40
829/829 [==============================] - 14s 17ms/step - loss: 0.8974 - accuracy: 0.5139 - val_loss: 1.3205 - val_accuracy: 0.4946
Epoch 28/40
829/829 [==============================] - 14s 17ms/step - loss: 0.8663 - accuracy: 0.5042 - val_loss: 1.4609 - val_accuracy: 0.4657
Epoch 29/40
829/829 [==============================] - 15s 17ms/step - loss: 0.8372 - accuracy: 0.5404 - val_loss: 1.1364 - val_accuracy: 0.4874
Epoch 30/40
829/829 [==============================] - 15s 18ms/step - loss: 0.7965 - accuracy: 0.5344 - val_loss: 2.7651 - val_accuracy: 0.4946
Epoch 31/40
829/829 [==============================] - 15s 18ms/step - loss: 0.7794 - accuracy: 0.5633 - val_loss: 1.0393 - val_accuracy: 0.5379
Epoch 32/40
829/829 [==============================] - 15s 18ms/step - loss: 0.7651 - accuracy: 0.5959 - val_loss: 1.7666 - val_accuracy: 0.4621
Epoch 33/40
829/829 [==============================] - 15s 18ms/step - loss: 0.7173 - accuracy: 0.6671 - val_loss: 2.4147 - val_accuracy: 0.4621
Epoch 34/40
829/829 [==============================] - 15s 18ms/step - loss: 0.6985 - accuracy: 0.6731 - val_loss: 1.9983 - val_accuracy: 0.4729
Epoch 35/40
829/829 [==============================] - 14s 17ms/step - loss: 0.6956 - accuracy: 0.7081 - val_loss: 3.1167 - val_accuracy: 0.4585
Epoch 36/40
829/829 [==============================] - 15s 18ms/step - loss: 0.6369 - accuracy: 0.7165 - val_loss: 1.8084 - val_accuracy: 0.4224
Epoch 37/40
829/829 [==============================] - 15s 18ms/step - loss: 0.5775 - accuracy: 0.7660 - val_loss: 1.4000 - val_accuracy: 0.4729
Epoch 38/40
829/829 [==============================] - 15s 18ms/step - loss: 0.5752 - accuracy: 0.7503 - val_loss: 1.7759 - val_accuracy: 0.4188
Epoch 39/40
829/829 [==============================] - 14s 17ms/step - loss: 0.5381 - accuracy: 0.7853 - val_loss: 2.1087 - val_accuracy: 0.4801
Epoch 40/40
829/829 [==============================] - 15s 18ms/step - loss: 0.4921 - accuracy: 0.7998 - val_loss: 2.5755 - val_accuracy: 0.4946
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">history_dict</span> <span class="o">=</span> <span class="n">history</span><span class="p">.</span><span class="n">history</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s">'loss'</span><span class="p">]</span>
<span class="n">val_loss</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">]</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s">'bo'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Validation loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Training and validation loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Epochs'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/img/movie_posters/output_58_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">acc</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">]</span>
<span class="n">val_acc</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s">'val_accuracy'</span><span class="p">]</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="s">'bo'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training acc'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Validation acc'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Training and validation acc'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Epochs'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/img/movie_posters/output_59_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s">'/content/drive/Shared drives/personal/weights/resnet_model.31-0.5379.hdf5'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>277/277 [==============================] - 2s 6ms/step





[1.1417856199216325, 0.47653430700302124]
</code></pre></div></div>

<h3 id="c-inception-v3">c. Inception V3</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">build_inception</span><span class="p">():</span>
    <span class="c1"># keras 기본 모델 이용
</span>    <span class="n">base_model</span> <span class="o">=</span> <span class="n">inception_v3</span><span class="p">.</span><span class="n">InceptionV3</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">include_top</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span> <span class="p">,</span><span class="mi">3</span><span class="p">))</span>
    <span class="n">output_model</span> <span class="o">=</span> <span class="n">base_model</span><span class="p">.</span><span class="n">output</span>
    <span class="n">flatten_model</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">()(</span><span class="n">output_model</span><span class="p">)</span>
    <span class="n">dense_model</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">flatten_model</span><span class="p">)</span>
    <span class="n">final_model</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">)(</span><span class="n">dense_model</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">base_model</span><span class="p">.</span><span class="nb">input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">final_model</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizers</span><span class="p">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">),</span>
                <span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span>
                <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">build_inception</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="s">'inception_model'</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "inception_model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_12 (InputLayer)           (None, 256, 256, 3)  0                                            
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 127, 127, 32) 864         input_12[0][0]                   
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 127, 127, 32) 96          conv2d_53[0][0]                  
__________________________________________________________________________________________________
activation_295 (Activation)     (None, 127, 127, 32) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 125, 125, 32) 9216        activation_295[0][0]             
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 125, 125, 32) 96          conv2d_54[0][0]                  
__________________________________________________________________________________________________
activation_296 (Activation)     (None, 125, 125, 32) 0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 125, 125, 64) 18432       activation_296[0][0]             
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 125, 125, 64) 192         conv2d_55[0][0]                  
__________________________________________________________________________________________________
activation_297 (Activation)     (None, 125, 125, 64) 0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
max_pooling2d_18 (MaxPooling2D) (None, 62, 62, 64)   0           activation_297[0][0]             
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 62, 62, 80)   5120        max_pooling2d_18[0][0]           
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 62, 62, 80)   240         conv2d_56[0][0]                  
__________________________________________________________________________________________________
activation_298 (Activation)     (None, 62, 62, 80)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 60, 60, 192)  138240      activation_298[0][0]             
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 60, 60, 192)  576         conv2d_57[0][0]                  
__________________________________________________________________________________________________
activation_299 (Activation)     (None, 60, 60, 192)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d_19 (MaxPooling2D) (None, 29, 29, 192)  0           activation_299[0][0]             
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 29, 29, 64)   12288       max_pooling2d_19[0][0]           
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 29, 29, 64)   192         conv2d_61[0][0]                  
__________________________________________________________________________________________________
activation_303 (Activation)     (None, 29, 29, 64)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 29, 29, 48)   9216        max_pooling2d_19[0][0]           
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 29, 29, 96)   55296       activation_303[0][0]             
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 29, 29, 48)   144         conv2d_59[0][0]                  
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 29, 29, 96)   288         conv2d_62[0][0]                  
__________________________________________________________________________________________________
activation_301 (Activation)     (None, 29, 29, 48)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
activation_304 (Activation)     (None, 29, 29, 96)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 29, 29, 192)  0           max_pooling2d_19[0][0]           
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 29, 29, 64)   12288       max_pooling2d_19[0][0]           
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 29, 29, 64)   76800       activation_301[0][0]             
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 29, 29, 96)   82944       activation_304[0][0]             
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 29, 29, 32)   6144        average_pooling2d_1[0][0]        
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 29, 29, 64)   192         conv2d_58[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 29, 29, 64)   192         conv2d_60[0][0]                  
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 29, 29, 96)   288         conv2d_63[0][0]                  
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 29, 29, 32)   96          conv2d_64[0][0]                  
__________________________________________________________________________________________________
activation_300 (Activation)     (None, 29, 29, 64)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
activation_302 (Activation)     (None, 29, 29, 64)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
activation_305 (Activation)     (None, 29, 29, 96)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
activation_306 (Activation)     (None, 29, 29, 32)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
mixed0 (Concatenate)            (None, 29, 29, 256)  0           activation_300[0][0]             
                                                                 activation_302[0][0]             
                                                                 activation_305[0][0]             
                                                                 activation_306[0][0]             
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 29, 29, 64)   16384       mixed0[0][0]                     
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 29, 29, 64)   192         conv2d_68[0][0]                  
__________________________________________________________________________________________________
activation_310 (Activation)     (None, 29, 29, 64)   0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 29, 29, 48)   12288       mixed0[0][0]                     
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 29, 29, 96)   55296       activation_310[0][0]             
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 29, 29, 48)   144         conv2d_66[0][0]                  
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 29, 29, 96)   288         conv2d_69[0][0]                  
__________________________________________________________________________________________________
activation_308 (Activation)     (None, 29, 29, 48)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
activation_311 (Activation)     (None, 29, 29, 96)   0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
average_pooling2d_2 (AveragePoo (None, 29, 29, 256)  0           mixed0[0][0]                     
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 29, 29, 64)   16384       mixed0[0][0]                     
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 29, 29, 64)   76800       activation_308[0][0]             
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 29, 29, 96)   82944       activation_311[0][0]             
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 29, 29, 64)   16384       average_pooling2d_2[0][0]        
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 29, 29, 64)   192         conv2d_65[0][0]                  
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 29, 29, 64)   192         conv2d_67[0][0]                  
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 29, 29, 96)   288         conv2d_70[0][0]                  
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 29, 29, 64)   192         conv2d_71[0][0]                  
__________________________________________________________________________________________________
activation_307 (Activation)     (None, 29, 29, 64)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
activation_309 (Activation)     (None, 29, 29, 64)   0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
activation_312 (Activation)     (None, 29, 29, 96)   0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
activation_313 (Activation)     (None, 29, 29, 64)   0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
mixed1 (Concatenate)            (None, 29, 29, 288)  0           activation_307[0][0]             
                                                                 activation_309[0][0]             
                                                                 activation_312[0][0]             
                                                                 activation_313[0][0]             
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 29, 29, 64)   18432       mixed1[0][0]                     
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 29, 29, 64)   192         conv2d_75[0][0]                  
__________________________________________________________________________________________________
activation_317 (Activation)     (None, 29, 29, 64)   0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 29, 29, 48)   13824       mixed1[0][0]                     
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 29, 29, 96)   55296       activation_317[0][0]             
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 29, 29, 48)   144         conv2d_73[0][0]                  
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 29, 29, 96)   288         conv2d_76[0][0]                  
__________________________________________________________________________________________________
activation_315 (Activation)     (None, 29, 29, 48)   0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
activation_318 (Activation)     (None, 29, 29, 96)   0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
average_pooling2d_3 (AveragePoo (None, 29, 29, 288)  0           mixed1[0][0]                     
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 29, 29, 64)   18432       mixed1[0][0]                     
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 29, 29, 64)   76800       activation_315[0][0]             
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 29, 29, 96)   82944       activation_318[0][0]             
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 29, 29, 64)   18432       average_pooling2d_3[0][0]        
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 29, 29, 64)   192         conv2d_72[0][0]                  
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 29, 29, 64)   192         conv2d_74[0][0]                  
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 29, 29, 96)   288         conv2d_77[0][0]                  
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 29, 29, 64)   192         conv2d_78[0][0]                  
__________________________________________________________________________________________________
activation_314 (Activation)     (None, 29, 29, 64)   0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
activation_316 (Activation)     (None, 29, 29, 64)   0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
activation_319 (Activation)     (None, 29, 29, 96)   0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
activation_320 (Activation)     (None, 29, 29, 64)   0           batch_normalization_26[0][0]     
__________________________________________________________________________________________________
mixed2 (Concatenate)            (None, 29, 29, 288)  0           activation_314[0][0]             
                                                                 activation_316[0][0]             
                                                                 activation_319[0][0]             
                                                                 activation_320[0][0]             
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 29, 29, 64)   18432       mixed2[0][0]                     
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 29, 29, 64)   192         conv2d_80[0][0]                  
__________________________________________________________________________________________________
activation_322 (Activation)     (None, 29, 29, 64)   0           batch_normalization_28[0][0]     
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 29, 29, 96)   55296       activation_322[0][0]             
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 29, 29, 96)   288         conv2d_81[0][0]                  
__________________________________________________________________________________________________
activation_323 (Activation)     (None, 29, 29, 96)   0           batch_normalization_29[0][0]     
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 14, 14, 384)  995328      mixed2[0][0]                     
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 14, 14, 96)   82944       activation_323[0][0]             
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 14, 14, 384)  1152        conv2d_79[0][0]                  
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 14, 14, 96)   288         conv2d_82[0][0]                  
__________________________________________________________________________________________________
activation_321 (Activation)     (None, 14, 14, 384)  0           batch_normalization_27[0][0]     
__________________________________________________________________________________________________
activation_324 (Activation)     (None, 14, 14, 96)   0           batch_normalization_30[0][0]     
__________________________________________________________________________________________________
max_pooling2d_20 (MaxPooling2D) (None, 14, 14, 288)  0           mixed2[0][0]                     
__________________________________________________________________________________________________
mixed3 (Concatenate)            (None, 14, 14, 768)  0           activation_321[0][0]             
                                                                 activation_324[0][0]             
                                                                 max_pooling2d_20[0][0]           
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 14, 14, 128)  98304       mixed3[0][0]                     
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 14, 14, 128)  384         conv2d_87[0][0]                  
__________________________________________________________________________________________________
activation_329 (Activation)     (None, 14, 14, 128)  0           batch_normalization_35[0][0]     
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 14, 14, 128)  114688      activation_329[0][0]             
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 14, 14, 128)  384         conv2d_88[0][0]                  
__________________________________________________________________________________________________
activation_330 (Activation)     (None, 14, 14, 128)  0           batch_normalization_36[0][0]     
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 14, 14, 128)  98304       mixed3[0][0]                     
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 14, 14, 128)  114688      activation_330[0][0]             
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 14, 14, 128)  384         conv2d_84[0][0]                  
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 14, 14, 128)  384         conv2d_89[0][0]                  
__________________________________________________________________________________________________
activation_326 (Activation)     (None, 14, 14, 128)  0           batch_normalization_32[0][0]     
__________________________________________________________________________________________________
activation_331 (Activation)     (None, 14, 14, 128)  0           batch_normalization_37[0][0]     
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 14, 14, 128)  114688      activation_326[0][0]             
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 14, 14, 128)  114688      activation_331[0][0]             
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 14, 14, 128)  384         conv2d_85[0][0]                  
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 14, 14, 128)  384         conv2d_90[0][0]                  
__________________________________________________________________________________________________
activation_327 (Activation)     (None, 14, 14, 128)  0           batch_normalization_33[0][0]     
__________________________________________________________________________________________________
activation_332 (Activation)     (None, 14, 14, 128)  0           batch_normalization_38[0][0]     
__________________________________________________________________________________________________
average_pooling2d_4 (AveragePoo (None, 14, 14, 768)  0           mixed3[0][0]                     
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 14, 14, 192)  147456      mixed3[0][0]                     
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 14, 14, 192)  172032      activation_327[0][0]             
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 14, 14, 192)  172032      activation_332[0][0]             
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 14, 14, 192)  147456      average_pooling2d_4[0][0]        
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 14, 14, 192)  576         conv2d_83[0][0]                  
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 14, 14, 192)  576         conv2d_86[0][0]                  
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 14, 14, 192)  576         conv2d_91[0][0]                  
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 14, 14, 192)  576         conv2d_92[0][0]                  
__________________________________________________________________________________________________
activation_325 (Activation)     (None, 14, 14, 192)  0           batch_normalization_31[0][0]     
__________________________________________________________________________________________________
activation_328 (Activation)     (None, 14, 14, 192)  0           batch_normalization_34[0][0]     
__________________________________________________________________________________________________
activation_333 (Activation)     (None, 14, 14, 192)  0           batch_normalization_39[0][0]     
__________________________________________________________________________________________________
activation_334 (Activation)     (None, 14, 14, 192)  0           batch_normalization_40[0][0]     
__________________________________________________________________________________________________
mixed4 (Concatenate)            (None, 14, 14, 768)  0           activation_325[0][0]             
                                                                 activation_328[0][0]             
                                                                 activation_333[0][0]             
                                                                 activation_334[0][0]             
__________________________________________________________________________________________________
conv2d_97 (Conv2D)              (None, 14, 14, 160)  122880      mixed4[0][0]                     
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 14, 14, 160)  480         conv2d_97[0][0]                  
__________________________________________________________________________________________________
activation_339 (Activation)     (None, 14, 14, 160)  0           batch_normalization_45[0][0]     
__________________________________________________________________________________________________
conv2d_98 (Conv2D)              (None, 14, 14, 160)  179200      activation_339[0][0]             
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 14, 14, 160)  480         conv2d_98[0][0]                  
__________________________________________________________________________________________________
activation_340 (Activation)     (None, 14, 14, 160)  0           batch_normalization_46[0][0]     
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 14, 14, 160)  122880      mixed4[0][0]                     
__________________________________________________________________________________________________
conv2d_99 (Conv2D)              (None, 14, 14, 160)  179200      activation_340[0][0]             
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 14, 14, 160)  480         conv2d_94[0][0]                  
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 14, 14, 160)  480         conv2d_99[0][0]                  
__________________________________________________________________________________________________
activation_336 (Activation)     (None, 14, 14, 160)  0           batch_normalization_42[0][0]     
__________________________________________________________________________________________________
activation_341 (Activation)     (None, 14, 14, 160)  0           batch_normalization_47[0][0]     
__________________________________________________________________________________________________
conv2d_95 (Conv2D)              (None, 14, 14, 160)  179200      activation_336[0][0]             
__________________________________________________________________________________________________
conv2d_100 (Conv2D)             (None, 14, 14, 160)  179200      activation_341[0][0]             
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 14, 14, 160)  480         conv2d_95[0][0]                  
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 14, 14, 160)  480         conv2d_100[0][0]                 
__________________________________________________________________________________________________
activation_337 (Activation)     (None, 14, 14, 160)  0           batch_normalization_43[0][0]     
__________________________________________________________________________________________________
activation_342 (Activation)     (None, 14, 14, 160)  0           batch_normalization_48[0][0]     
__________________________________________________________________________________________________
average_pooling2d_5 (AveragePoo (None, 14, 14, 768)  0           mixed4[0][0]                     
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 14, 14, 192)  147456      mixed4[0][0]                     
__________________________________________________________________________________________________
conv2d_96 (Conv2D)              (None, 14, 14, 192)  215040      activation_337[0][0]             
__________________________________________________________________________________________________
conv2d_101 (Conv2D)             (None, 14, 14, 192)  215040      activation_342[0][0]             
__________________________________________________________________________________________________
conv2d_102 (Conv2D)             (None, 14, 14, 192)  147456      average_pooling2d_5[0][0]        
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 14, 14, 192)  576         conv2d_93[0][0]                  
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 14, 14, 192)  576         conv2d_96[0][0]                  
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 14, 14, 192)  576         conv2d_101[0][0]                 
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 14, 14, 192)  576         conv2d_102[0][0]                 
__________________________________________________________________________________________________
activation_335 (Activation)     (None, 14, 14, 192)  0           batch_normalization_41[0][0]     
__________________________________________________________________________________________________
activation_338 (Activation)     (None, 14, 14, 192)  0           batch_normalization_44[0][0]     
__________________________________________________________________________________________________
activation_343 (Activation)     (None, 14, 14, 192)  0           batch_normalization_49[0][0]     
__________________________________________________________________________________________________
activation_344 (Activation)     (None, 14, 14, 192)  0           batch_normalization_50[0][0]     
__________________________________________________________________________________________________
mixed5 (Concatenate)            (None, 14, 14, 768)  0           activation_335[0][0]             
                                                                 activation_338[0][0]             
                                                                 activation_343[0][0]             
                                                                 activation_344[0][0]             
__________________________________________________________________________________________________
conv2d_107 (Conv2D)             (None, 14, 14, 160)  122880      mixed5[0][0]                     
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 14, 14, 160)  480         conv2d_107[0][0]                 
__________________________________________________________________________________________________
activation_349 (Activation)     (None, 14, 14, 160)  0           batch_normalization_55[0][0]     
__________________________________________________________________________________________________
conv2d_108 (Conv2D)             (None, 14, 14, 160)  179200      activation_349[0][0]             
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 14, 14, 160)  480         conv2d_108[0][0]                 
__________________________________________________________________________________________________
activation_350 (Activation)     (None, 14, 14, 160)  0           batch_normalization_56[0][0]     
__________________________________________________________________________________________________
conv2d_104 (Conv2D)             (None, 14, 14, 160)  122880      mixed5[0][0]                     
__________________________________________________________________________________________________
conv2d_109 (Conv2D)             (None, 14, 14, 160)  179200      activation_350[0][0]             
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 14, 14, 160)  480         conv2d_104[0][0]                 
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 14, 14, 160)  480         conv2d_109[0][0]                 
__________________________________________________________________________________________________
activation_346 (Activation)     (None, 14, 14, 160)  0           batch_normalization_52[0][0]     
__________________________________________________________________________________________________
activation_351 (Activation)     (None, 14, 14, 160)  0           batch_normalization_57[0][0]     
__________________________________________________________________________________________________
conv2d_105 (Conv2D)             (None, 14, 14, 160)  179200      activation_346[0][0]             
__________________________________________________________________________________________________
conv2d_110 (Conv2D)             (None, 14, 14, 160)  179200      activation_351[0][0]             
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 14, 14, 160)  480         conv2d_105[0][0]                 
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 14, 14, 160)  480         conv2d_110[0][0]                 
__________________________________________________________________________________________________
activation_347 (Activation)     (None, 14, 14, 160)  0           batch_normalization_53[0][0]     
__________________________________________________________________________________________________
activation_352 (Activation)     (None, 14, 14, 160)  0           batch_normalization_58[0][0]     
__________________________________________________________________________________________________
average_pooling2d_6 (AveragePoo (None, 14, 14, 768)  0           mixed5[0][0]                     
__________________________________________________________________________________________________
conv2d_103 (Conv2D)             (None, 14, 14, 192)  147456      mixed5[0][0]                     
__________________________________________________________________________________________________
conv2d_106 (Conv2D)             (None, 14, 14, 192)  215040      activation_347[0][0]             
__________________________________________________________________________________________________
conv2d_111 (Conv2D)             (None, 14, 14, 192)  215040      activation_352[0][0]             
__________________________________________________________________________________________________
conv2d_112 (Conv2D)             (None, 14, 14, 192)  147456      average_pooling2d_6[0][0]        
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 14, 14, 192)  576         conv2d_103[0][0]                 
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 14, 14, 192)  576         conv2d_106[0][0]                 
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 14, 14, 192)  576         conv2d_111[0][0]                 
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 14, 14, 192)  576         conv2d_112[0][0]                 
__________________________________________________________________________________________________
activation_345 (Activation)     (None, 14, 14, 192)  0           batch_normalization_51[0][0]     
__________________________________________________________________________________________________
activation_348 (Activation)     (None, 14, 14, 192)  0           batch_normalization_54[0][0]     
__________________________________________________________________________________________________
activation_353 (Activation)     (None, 14, 14, 192)  0           batch_normalization_59[0][0]     
__________________________________________________________________________________________________
activation_354 (Activation)     (None, 14, 14, 192)  0           batch_normalization_60[0][0]     
__________________________________________________________________________________________________
mixed6 (Concatenate)            (None, 14, 14, 768)  0           activation_345[0][0]             
                                                                 activation_348[0][0]             
                                                                 activation_353[0][0]             
                                                                 activation_354[0][0]             
__________________________________________________________________________________________________
conv2d_117 (Conv2D)             (None, 14, 14, 192)  147456      mixed6[0][0]                     
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 14, 14, 192)  576         conv2d_117[0][0]                 
__________________________________________________________________________________________________
activation_359 (Activation)     (None, 14, 14, 192)  0           batch_normalization_65[0][0]     
__________________________________________________________________________________________________
conv2d_118 (Conv2D)             (None, 14, 14, 192)  258048      activation_359[0][0]             
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 14, 14, 192)  576         conv2d_118[0][0]                 
__________________________________________________________________________________________________
activation_360 (Activation)     (None, 14, 14, 192)  0           batch_normalization_66[0][0]     
__________________________________________________________________________________________________
conv2d_114 (Conv2D)             (None, 14, 14, 192)  147456      mixed6[0][0]                     
__________________________________________________________________________________________________
conv2d_119 (Conv2D)             (None, 14, 14, 192)  258048      activation_360[0][0]             
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 14, 14, 192)  576         conv2d_114[0][0]                 
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 14, 14, 192)  576         conv2d_119[0][0]                 
__________________________________________________________________________________________________
activation_356 (Activation)     (None, 14, 14, 192)  0           batch_normalization_62[0][0]     
__________________________________________________________________________________________________
activation_361 (Activation)     (None, 14, 14, 192)  0           batch_normalization_67[0][0]     
__________________________________________________________________________________________________
conv2d_115 (Conv2D)             (None, 14, 14, 192)  258048      activation_356[0][0]             
__________________________________________________________________________________________________
conv2d_120 (Conv2D)             (None, 14, 14, 192)  258048      activation_361[0][0]             
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 14, 14, 192)  576         conv2d_115[0][0]                 
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 14, 14, 192)  576         conv2d_120[0][0]                 
__________________________________________________________________________________________________
activation_357 (Activation)     (None, 14, 14, 192)  0           batch_normalization_63[0][0]     
__________________________________________________________________________________________________
activation_362 (Activation)     (None, 14, 14, 192)  0           batch_normalization_68[0][0]     
__________________________________________________________________________________________________
average_pooling2d_7 (AveragePoo (None, 14, 14, 768)  0           mixed6[0][0]                     
__________________________________________________________________________________________________
conv2d_113 (Conv2D)             (None, 14, 14, 192)  147456      mixed6[0][0]                     
__________________________________________________________________________________________________
conv2d_116 (Conv2D)             (None, 14, 14, 192)  258048      activation_357[0][0]             
__________________________________________________________________________________________________
conv2d_121 (Conv2D)             (None, 14, 14, 192)  258048      activation_362[0][0]             
__________________________________________________________________________________________________
conv2d_122 (Conv2D)             (None, 14, 14, 192)  147456      average_pooling2d_7[0][0]        
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 14, 14, 192)  576         conv2d_113[0][0]                 
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 14, 14, 192)  576         conv2d_116[0][0]                 
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 14, 14, 192)  576         conv2d_121[0][0]                 
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 14, 14, 192)  576         conv2d_122[0][0]                 
__________________________________________________________________________________________________
activation_355 (Activation)     (None, 14, 14, 192)  0           batch_normalization_61[0][0]     
__________________________________________________________________________________________________
activation_358 (Activation)     (None, 14, 14, 192)  0           batch_normalization_64[0][0]     
__________________________________________________________________________________________________
activation_363 (Activation)     (None, 14, 14, 192)  0           batch_normalization_69[0][0]     
__________________________________________________________________________________________________
activation_364 (Activation)     (None, 14, 14, 192)  0           batch_normalization_70[0][0]     
__________________________________________________________________________________________________
mixed7 (Concatenate)            (None, 14, 14, 768)  0           activation_355[0][0]             
                                                                 activation_358[0][0]             
                                                                 activation_363[0][0]             
                                                                 activation_364[0][0]             
__________________________________________________________________________________________________
conv2d_125 (Conv2D)             (None, 14, 14, 192)  147456      mixed7[0][0]                     
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 14, 14, 192)  576         conv2d_125[0][0]                 
__________________________________________________________________________________________________
activation_367 (Activation)     (None, 14, 14, 192)  0           batch_normalization_73[0][0]     
__________________________________________________________________________________________________
conv2d_126 (Conv2D)             (None, 14, 14, 192)  258048      activation_367[0][0]             
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 14, 14, 192)  576         conv2d_126[0][0]                 
__________________________________________________________________________________________________
activation_368 (Activation)     (None, 14, 14, 192)  0           batch_normalization_74[0][0]     
__________________________________________________________________________________________________
conv2d_123 (Conv2D)             (None, 14, 14, 192)  147456      mixed7[0][0]                     
__________________________________________________________________________________________________
conv2d_127 (Conv2D)             (None, 14, 14, 192)  258048      activation_368[0][0]             
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 14, 14, 192)  576         conv2d_123[0][0]                 
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 14, 14, 192)  576         conv2d_127[0][0]                 
__________________________________________________________________________________________________
activation_365 (Activation)     (None, 14, 14, 192)  0           batch_normalization_71[0][0]     
__________________________________________________________________________________________________
activation_369 (Activation)     (None, 14, 14, 192)  0           batch_normalization_75[0][0]     
__________________________________________________________________________________________________
conv2d_124 (Conv2D)             (None, 6, 6, 320)    552960      activation_365[0][0]             
__________________________________________________________________________________________________
conv2d_128 (Conv2D)             (None, 6, 6, 192)    331776      activation_369[0][0]             
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 6, 6, 320)    960         conv2d_124[0][0]                 
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 6, 6, 192)    576         conv2d_128[0][0]                 
__________________________________________________________________________________________________
activation_366 (Activation)     (None, 6, 6, 320)    0           batch_normalization_72[0][0]     
__________________________________________________________________________________________________
activation_370 (Activation)     (None, 6, 6, 192)    0           batch_normalization_76[0][0]     
__________________________________________________________________________________________________
max_pooling2d_21 (MaxPooling2D) (None, 6, 6, 768)    0           mixed7[0][0]                     
__________________________________________________________________________________________________
mixed8 (Concatenate)            (None, 6, 6, 1280)   0           activation_366[0][0]             
                                                                 activation_370[0][0]             
                                                                 max_pooling2d_21[0][0]           
__________________________________________________________________________________________________
conv2d_133 (Conv2D)             (None, 6, 6, 448)    573440      mixed8[0][0]                     
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 6, 6, 448)    1344        conv2d_133[0][0]                 
__________________________________________________________________________________________________
activation_375 (Activation)     (None, 6, 6, 448)    0           batch_normalization_81[0][0]     
__________________________________________________________________________________________________
conv2d_130 (Conv2D)             (None, 6, 6, 384)    491520      mixed8[0][0]                     
__________________________________________________________________________________________________
conv2d_134 (Conv2D)             (None, 6, 6, 384)    1548288     activation_375[0][0]             
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 6, 6, 384)    1152        conv2d_130[0][0]                 
__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 6, 6, 384)    1152        conv2d_134[0][0]                 
__________________________________________________________________________________________________
activation_372 (Activation)     (None, 6, 6, 384)    0           batch_normalization_78[0][0]     
__________________________________________________________________________________________________
activation_376 (Activation)     (None, 6, 6, 384)    0           batch_normalization_82[0][0]     
__________________________________________________________________________________________________
conv2d_131 (Conv2D)             (None, 6, 6, 384)    442368      activation_372[0][0]             
__________________________________________________________________________________________________
conv2d_132 (Conv2D)             (None, 6, 6, 384)    442368      activation_372[0][0]             
__________________________________________________________________________________________________
conv2d_135 (Conv2D)             (None, 6, 6, 384)    442368      activation_376[0][0]             
__________________________________________________________________________________________________
conv2d_136 (Conv2D)             (None, 6, 6, 384)    442368      activation_376[0][0]             
__________________________________________________________________________________________________
average_pooling2d_8 (AveragePoo (None, 6, 6, 1280)   0           mixed8[0][0]                     
__________________________________________________________________________________________________
conv2d_129 (Conv2D)             (None, 6, 6, 320)    409600      mixed8[0][0]                     
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 6, 6, 384)    1152        conv2d_131[0][0]                 
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 6, 6, 384)    1152        conv2d_132[0][0]                 
__________________________________________________________________________________________________
batch_normalization_83 (BatchNo (None, 6, 6, 384)    1152        conv2d_135[0][0]                 
__________________________________________________________________________________________________
batch_normalization_84 (BatchNo (None, 6, 6, 384)    1152        conv2d_136[0][0]                 
__________________________________________________________________________________________________
conv2d_137 (Conv2D)             (None, 6, 6, 192)    245760      average_pooling2d_8[0][0]        
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 6, 6, 320)    960         conv2d_129[0][0]                 
__________________________________________________________________________________________________
activation_373 (Activation)     (None, 6, 6, 384)    0           batch_normalization_79[0][0]     
__________________________________________________________________________________________________
activation_374 (Activation)     (None, 6, 6, 384)    0           batch_normalization_80[0][0]     
__________________________________________________________________________________________________
activation_377 (Activation)     (None, 6, 6, 384)    0           batch_normalization_83[0][0]     
__________________________________________________________________________________________________
activation_378 (Activation)     (None, 6, 6, 384)    0           batch_normalization_84[0][0]     
__________________________________________________________________________________________________
batch_normalization_85 (BatchNo (None, 6, 6, 192)    576         conv2d_137[0][0]                 
__________________________________________________________________________________________________
activation_371 (Activation)     (None, 6, 6, 320)    0           batch_normalization_77[0][0]     
__________________________________________________________________________________________________
mixed9_0 (Concatenate)          (None, 6, 6, 768)    0           activation_373[0][0]             
                                                                 activation_374[0][0]             
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 6, 6, 768)    0           activation_377[0][0]             
                                                                 activation_378[0][0]             
__________________________________________________________________________________________________
activation_379 (Activation)     (None, 6, 6, 192)    0           batch_normalization_85[0][0]     
__________________________________________________________________________________________________
mixed9 (Concatenate)            (None, 6, 6, 2048)   0           activation_371[0][0]             
                                                                 mixed9_0[0][0]                   
                                                                 concatenate_9[0][0]              
                                                                 activation_379[0][0]             
__________________________________________________________________________________________________
conv2d_142 (Conv2D)             (None, 6, 6, 448)    917504      mixed9[0][0]                     
__________________________________________________________________________________________________
batch_normalization_90 (BatchNo (None, 6, 6, 448)    1344        conv2d_142[0][0]                 
__________________________________________________________________________________________________
activation_384 (Activation)     (None, 6, 6, 448)    0           batch_normalization_90[0][0]     
__________________________________________________________________________________________________
conv2d_139 (Conv2D)             (None, 6, 6, 384)    786432      mixed9[0][0]                     
__________________________________________________________________________________________________
conv2d_143 (Conv2D)             (None, 6, 6, 384)    1548288     activation_384[0][0]             
__________________________________________________________________________________________________
batch_normalization_87 (BatchNo (None, 6, 6, 384)    1152        conv2d_139[0][0]                 
__________________________________________________________________________________________________
batch_normalization_91 (BatchNo (None, 6, 6, 384)    1152        conv2d_143[0][0]                 
__________________________________________________________________________________________________
activation_381 (Activation)     (None, 6, 6, 384)    0           batch_normalization_87[0][0]     
__________________________________________________________________________________________________
activation_385 (Activation)     (None, 6, 6, 384)    0           batch_normalization_91[0][0]     
__________________________________________________________________________________________________
conv2d_140 (Conv2D)             (None, 6, 6, 384)    442368      activation_381[0][0]             
__________________________________________________________________________________________________
conv2d_141 (Conv2D)             (None, 6, 6, 384)    442368      activation_381[0][0]             
__________________________________________________________________________________________________
conv2d_144 (Conv2D)             (None, 6, 6, 384)    442368      activation_385[0][0]             
__________________________________________________________________________________________________
conv2d_145 (Conv2D)             (None, 6, 6, 384)    442368      activation_385[0][0]             
__________________________________________________________________________________________________
average_pooling2d_9 (AveragePoo (None, 6, 6, 2048)   0           mixed9[0][0]                     
__________________________________________________________________________________________________
conv2d_138 (Conv2D)             (None, 6, 6, 320)    655360      mixed9[0][0]                     
__________________________________________________________________________________________________
batch_normalization_88 (BatchNo (None, 6, 6, 384)    1152        conv2d_140[0][0]                 
__________________________________________________________________________________________________
batch_normalization_89 (BatchNo (None, 6, 6, 384)    1152        conv2d_141[0][0]                 
__________________________________________________________________________________________________
batch_normalization_92 (BatchNo (None, 6, 6, 384)    1152        conv2d_144[0][0]                 
__________________________________________________________________________________________________
batch_normalization_93 (BatchNo (None, 6, 6, 384)    1152        conv2d_145[0][0]                 
__________________________________________________________________________________________________
conv2d_146 (Conv2D)             (None, 6, 6, 192)    393216      average_pooling2d_9[0][0]        
__________________________________________________________________________________________________
batch_normalization_86 (BatchNo (None, 6, 6, 320)    960         conv2d_138[0][0]                 
__________________________________________________________________________________________________
activation_382 (Activation)     (None, 6, 6, 384)    0           batch_normalization_88[0][0]     
__________________________________________________________________________________________________
activation_383 (Activation)     (None, 6, 6, 384)    0           batch_normalization_89[0][0]     
__________________________________________________________________________________________________
activation_386 (Activation)     (None, 6, 6, 384)    0           batch_normalization_92[0][0]     
__________________________________________________________________________________________________
activation_387 (Activation)     (None, 6, 6, 384)    0           batch_normalization_93[0][0]     
__________________________________________________________________________________________________
batch_normalization_94 (BatchNo (None, 6, 6, 192)    576         conv2d_146[0][0]                 
__________________________________________________________________________________________________
activation_380 (Activation)     (None, 6, 6, 320)    0           batch_normalization_86[0][0]     
__________________________________________________________________________________________________
mixed9_1 (Concatenate)          (None, 6, 6, 768)    0           activation_382[0][0]             
                                                                 activation_383[0][0]             
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 6, 6, 768)    0           activation_386[0][0]             
                                                                 activation_387[0][0]             
__________________________________________________________________________________________________
activation_388 (Activation)     (None, 6, 6, 192)    0           batch_normalization_94[0][0]     
__________________________________________________________________________________________________
mixed10 (Concatenate)           (None, 6, 6, 2048)   0           activation_380[0][0]             
                                                                 mixed9_1[0][0]                   
                                                                 concatenate_10[0][0]             
                                                                 activation_388[0][0]             
__________________________________________________________________________________________________
flatten_8 (Flatten)             (None, 73728)        0           mixed10[0][0]                    
__________________________________________________________________________________________________
dense_18 (Dense)                (None, 512)          37749248    flatten_8[0][0]                  
__________________________________________________________________________________________________
dense_19 (Dense)                (None, 3)            1539        dense_18[0][0]                   
==================================================================================================
Total params: 59,553,571
Trainable params: 59,519,139
Non-trainable params: 34,432
__________________________________________________________________________________________________
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">history</span> <span class="o">=</span> <span class="n">model_train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="mi">40</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Train on 829 samples, validate on 277 samples
Epoch 1/40
829/829 [==============================] - 30s 36ms/step - loss: 3.7299 - accuracy: 0.3932 - val_loss: 1.0825 - val_accuracy: 0.4910
Epoch 2/40
829/829 [==============================] - 12s 14ms/step - loss: 1.2540 - accuracy: 0.4729 - val_loss: 1.0804 - val_accuracy: 0.4910
Epoch 3/40
829/829 [==============================] - 11s 14ms/step - loss: 1.1731 - accuracy: 0.4524 - val_loss: 1.1040 - val_accuracy: 0.4910
Epoch 4/40
829/829 [==============================] - 11s 14ms/step - loss: 1.0617 - accuracy: 0.4632 - val_loss: 1.0473 - val_accuracy: 0.4910
Epoch 5/40
829/829 [==============================] - 12s 14ms/step - loss: 1.0231 - accuracy: 0.5066 - val_loss: 1.0410 - val_accuracy: 0.4910
Epoch 6/40
829/829 [==============================] - 12s 14ms/step - loss: 0.9809 - accuracy: 0.5271 - val_loss: 1.0515 - val_accuracy: 0.4838
Epoch 7/40
829/829 [==============================] - 12s 14ms/step - loss: 0.9229 - accuracy: 0.5754 - val_loss: 1.0551 - val_accuracy: 0.4260
Epoch 8/40
829/829 [==============================] - 11s 14ms/step - loss: 0.8852 - accuracy: 0.6055 - val_loss: 1.0078 - val_accuracy: 0.4982
Epoch 9/40
829/829 [==============================] - 11s 14ms/step - loss: 0.8556 - accuracy: 0.6019 - val_loss: 1.1044 - val_accuracy: 0.4513
Epoch 10/40
829/829 [==============================] - 11s 14ms/step - loss: 0.8235 - accuracy: 0.6333 - val_loss: 1.2843 - val_accuracy: 0.5235
Epoch 11/40
829/829 [==============================] - 11s 14ms/step - loss: 0.7848 - accuracy: 0.6598 - val_loss: 1.2311 - val_accuracy: 0.4910
Epoch 12/40
829/829 [==============================] - 11s 14ms/step - loss: 0.7309 - accuracy: 0.6779 - val_loss: 1.2602 - val_accuracy: 0.4729
Epoch 13/40
829/829 [==============================] - 11s 14ms/step - loss: 0.6674 - accuracy: 0.7021 - val_loss: 1.3869 - val_accuracy: 0.5090
Epoch 14/40
829/829 [==============================] - 11s 14ms/step - loss: 0.5873 - accuracy: 0.7648 - val_loss: 2.0446 - val_accuracy: 0.4657
Epoch 15/40
829/829 [==============================] - 11s 14ms/step - loss: 0.5813 - accuracy: 0.7551 - val_loss: 1.8342 - val_accuracy: 0.5018
Epoch 16/40
829/829 [==============================] - 11s 14ms/step - loss: 0.5094 - accuracy: 0.7877 - val_loss: 1.5994 - val_accuracy: 0.5307
Epoch 17/40
829/829 [==============================] - 11s 14ms/step - loss: 0.4369 - accuracy: 0.8275 - val_loss: 2.0702 - val_accuracy: 0.4079
Epoch 18/40
829/829 [==============================] - 11s 14ms/step - loss: 0.3747 - accuracy: 0.8516 - val_loss: 1.7279 - val_accuracy: 0.4657
Epoch 19/40
829/829 [==============================] - 11s 14ms/step - loss: 0.3560 - accuracy: 0.8721 - val_loss: 2.1876 - val_accuracy: 0.5162
Epoch 20/40
829/829 [==============================] - 11s 14ms/step - loss: 0.3717 - accuracy: 0.8456 - val_loss: 1.7713 - val_accuracy: 0.4874
Epoch 21/40
829/829 [==============================] - 11s 14ms/step - loss: 0.2823 - accuracy: 0.8866 - val_loss: 2.7828 - val_accuracy: 0.4079
Epoch 22/40
829/829 [==============================] - 11s 14ms/step - loss: 0.2823 - accuracy: 0.8926 - val_loss: 2.1675 - val_accuracy: 0.4585
Epoch 23/40
829/829 [==============================] - 11s 14ms/step - loss: 0.2486 - accuracy: 0.9059 - val_loss: 1.9805 - val_accuracy: 0.4801
Epoch 24/40
829/829 [==============================] - 11s 14ms/step - loss: 0.2655 - accuracy: 0.9059 - val_loss: 2.0483 - val_accuracy: 0.5054
Epoch 25/40
829/829 [==============================] - 11s 14ms/step - loss: 0.2083 - accuracy: 0.9324 - val_loss: 2.6615 - val_accuracy: 0.4332
Epoch 26/40
829/829 [==============================] - 11s 14ms/step - loss: 0.1949 - accuracy: 0.9312 - val_loss: 2.8827 - val_accuracy: 0.4874
Epoch 27/40
829/829 [==============================] - 11s 14ms/step - loss: 0.1955 - accuracy: 0.9300 - val_loss: 2.4904 - val_accuracy: 0.5162
Epoch 28/40
829/829 [==============================] - 11s 14ms/step - loss: 0.1771 - accuracy: 0.9493 - val_loss: 3.3318 - val_accuracy: 0.4910
Epoch 29/40
829/829 [==============================] - 11s 14ms/step - loss: 0.2101 - accuracy: 0.9361 - val_loss: 3.1583 - val_accuracy: 0.4801
Epoch 30/40
829/829 [==============================] - 11s 14ms/step - loss: 0.2146 - accuracy: 0.9240 - val_loss: 3.4822 - val_accuracy: 0.4801
Epoch 31/40
829/829 [==============================] - 11s 14ms/step - loss: 0.2227 - accuracy: 0.9228 - val_loss: 2.6430 - val_accuracy: 0.4585
Epoch 32/40
829/829 [==============================] - 11s 14ms/step - loss: 0.1474 - accuracy: 0.9493 - val_loss: 2.4249 - val_accuracy: 0.4693
Epoch 33/40
829/829 [==============================] - 11s 14ms/step - loss: 0.1881 - accuracy: 0.9349 - val_loss: 2.9240 - val_accuracy: 0.4729
Epoch 34/40
829/829 [==============================] - 11s 14ms/step - loss: 0.1254 - accuracy: 0.9517 - val_loss: 2.9332 - val_accuracy: 0.4260
Epoch 35/40
829/829 [==============================] - 11s 14ms/step - loss: 0.1566 - accuracy: 0.9409 - val_loss: 3.0209 - val_accuracy: 0.5018
Epoch 36/40
829/829 [==============================] - 11s 14ms/step - loss: 0.1675 - accuracy: 0.9457 - val_loss: 3.3320 - val_accuracy: 0.4982
Epoch 37/40
829/829 [==============================] - 11s 14ms/step - loss: 0.1815 - accuracy: 0.9445 - val_loss: 2.4762 - val_accuracy: 0.4946
Epoch 38/40
829/829 [==============================] - 11s 14ms/step - loss: 0.1494 - accuracy: 0.9433 - val_loss: 3.1968 - val_accuracy: 0.4621
Epoch 39/40
829/829 [==============================] - 11s 14ms/step - loss: 0.0946 - accuracy: 0.9686 - val_loss: 3.6402 - val_accuracy: 0.4368
Epoch 40/40
829/829 [==============================] - 11s 14ms/step - loss: 0.1392 - accuracy: 0.9542 - val_loss: 3.6814 - val_accuracy: 0.4440
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">history_dict</span> <span class="o">=</span> <span class="n">history</span><span class="p">.</span><span class="n">history</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s">'loss'</span><span class="p">]</span>
<span class="n">val_loss</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">]</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s">'bo'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Validation loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Training and validation loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Epochs'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/img/movie_posters/output_68_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">acc</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">]</span>
<span class="n">val_acc</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s">'val_accuracy'</span><span class="p">]</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="s">'bo'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training acc'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Validation acc'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Training and validation acc'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Epochs'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/img/movie_posters/output_69_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s">'/content/drive/Shared drives/personal/weights/inception_model.16-0.5307.hdf5'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>277/277 [==============================] - 2s 6ms/step





[1.4880577596946745, 0.48736461997032166]
</code></pre></div></div>

<h3 id="d-비교를-위한-fully-connected-neural-network">d. 비교를 위한 Fully Connected Neural Network</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">build_fcnn</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
    <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
    <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
    <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
    <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">))</span>
    <span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizers</span><span class="p">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">),</span>
                  <span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span>
                  <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">build_fcnn</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="s">'fcnn_model'</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "fcnn_model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_12 (Flatten)         (None, 196608)            0         
_________________________________________________________________
dense_34 (Dense)             (None, 512)               100663808 
_________________________________________________________________
dense_35 (Dense)             (None, 512)               262656    
_________________________________________________________________
dense_36 (Dense)             (None, 512)               262656    
_________________________________________________________________
dense_37 (Dense)             (None, 3)                 1539      
=================================================================
Total params: 101,190,659
Trainable params: 101,190,659
Non-trainable params: 0
_________________________________________________________________
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">history</span> <span class="o">=</span> <span class="n">model_train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Train on 829 samples, validate on 277 samples
Epoch 1/20
829/829 [==============================] - 2s 2ms/step - loss: 8.2438 - accuracy: 0.3691 - val_loss: 2.3912 - val_accuracy: 0.4549
Epoch 2/20
829/829 [==============================] - 2s 2ms/step - loss: 3.4840 - accuracy: 0.3848 - val_loss: 2.9575 - val_accuracy: 0.4910
Epoch 3/20
829/829 [==============================] - 2s 2ms/step - loss: 2.1822 - accuracy: 0.3908 - val_loss: 2.1029 - val_accuracy: 0.1697
Epoch 4/20
829/829 [==============================] - 2s 2ms/step - loss: 1.5931 - accuracy: 0.4017 - val_loss: 1.0308 - val_accuracy: 0.4693
Epoch 5/20
829/829 [==============================] - 2s 2ms/step - loss: 1.2660 - accuracy: 0.4427 - val_loss: 1.1608 - val_accuracy: 0.3538
Epoch 6/20
829/829 [==============================] - 2s 2ms/step - loss: 1.1859 - accuracy: 0.4499 - val_loss: 1.1899 - val_accuracy: 0.4404
Epoch 7/20
829/829 [==============================] - 2s 2ms/step - loss: 1.1402 - accuracy: 0.4680 - val_loss: 1.0673 - val_accuracy: 0.4946
Epoch 8/20
829/829 [==============================] - 2s 2ms/step - loss: 1.0462 - accuracy: 0.5078 - val_loss: 1.1018 - val_accuracy: 0.4549
Epoch 9/20
829/829 [==============================] - 2s 2ms/step - loss: 1.0319 - accuracy: 0.4885 - val_loss: 1.1591 - val_accuracy: 0.3682
Epoch 10/20
829/829 [==============================] - 2s 2ms/step - loss: 1.0058 - accuracy: 0.5187 - val_loss: 1.0854 - val_accuracy: 0.4404
Epoch 11/20
829/829 [==============================] - 2s 2ms/step - loss: 1.0152 - accuracy: 0.5115 - val_loss: 1.3047 - val_accuracy: 0.3538
Epoch 12/20
829/829 [==============================] - 2s 2ms/step - loss: 0.9439 - accuracy: 0.5754 - val_loss: 1.1428 - val_accuracy: 0.4007
Epoch 13/20
829/829 [==============================] - 2s 2ms/step - loss: 0.9514 - accuracy: 0.5464 - val_loss: 1.2914 - val_accuracy: 0.4296
Epoch 14/20
829/829 [==============================] - 2s 2ms/step - loss: 0.9071 - accuracy: 0.5862 - val_loss: 1.6851 - val_accuracy: 0.4765
Epoch 15/20
829/829 [==============================] - 2s 2ms/step - loss: 0.8778 - accuracy: 0.6043 - val_loss: 1.2055 - val_accuracy: 0.3610
Epoch 16/20
829/829 [==============================] - 2s 2ms/step - loss: 0.8328 - accuracy: 0.6188 - val_loss: 1.6090 - val_accuracy: 0.3755
Epoch 17/20
829/829 [==============================] - 2s 2ms/step - loss: 0.8415 - accuracy: 0.6454 - val_loss: 1.5808 - val_accuracy: 0.4332
Epoch 18/20
829/829 [==============================] - 2s 2ms/step - loss: 0.7492 - accuracy: 0.6779 - val_loss: 1.5207 - val_accuracy: 0.3935
Epoch 19/20
829/829 [==============================] - 2s 2ms/step - loss: 0.7720 - accuracy: 0.6876 - val_loss: 1.4689 - val_accuracy: 0.4224
Epoch 20/20
829/829 [==============================] - 2s 2ms/step - loss: 0.7792 - accuracy: 0.6948 - val_loss: 1.2987 - val_accuracy: 0.4260
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">history_dict</span> <span class="o">=</span> <span class="n">history</span><span class="p">.</span><span class="n">history</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s">'loss'</span><span class="p">]</span>
<span class="n">val_loss</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">]</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s">'bo'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Validation loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Training and validation loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Epochs'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/img/movie_posters/output_78_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">acc</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">]</span>
<span class="n">val_acc</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s">'val_accuracy'</span><span class="p">]</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="s">'bo'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Training acc'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="s">'b'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Validation acc'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Training and validation acc'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Epochs'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/img/movie_posters/output_79_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">load_weights</span><span class="p">(</span><span class="s">'/content/drive/Shared drives/personal/weights/fcnn_model.07-0.4946.hdf5'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>277/277 [==============================] - 0s 520us/step





[1.0848816297544899, 0.45126354694366455]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

    



<div class="post-tags">
  
    
    <a href="/tags.html#python">
    
      <span class="icon">
        <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M0 0h24v24H0z" fill="none"/>
    <path d="M17.63 5.84C17.27 5.33 16.67 5 16 5L5 5.01C3.9 5.01 3 5.9 3 7v10c0 1.1.9 1.99 2 1.99L16 19c.67 0 1.27-.33 1.63-.84L22 12l-4.37-6.16z"/>
</svg>
      </span>&nbsp;<span class="tag-name">python</span>
    </a>
  
    
    <a href="/tags.html#deep-learning">
    
      <span class="icon">
        <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M0 0h24v24H0z" fill="none"/>
    <path d="M17.63 5.84C17.27 5.33 16.67 5 16 5L5 5.01C3.9 5.01 3 5.9 3 7v10c0 1.1.9 1.99 2 1.99L16 19c.67 0 1.27-.33 1.63-.84L22 12l-4.37-6.16z"/>
</svg>
      </span>&nbsp;<span class="tag-name">deep learning</span>
    </a>
  
    
    <a href="/tags.html#cnn">
    
      <span class="icon">
        <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M0 0h24v24H0z" fill="none"/>
    <path d="M17.63 5.84C17.27 5.33 16.67 5 16 5L5 5.01C3.9 5.01 3 5.9 3 7v10c0 1.1.9 1.99 2 1.99L16 19c.67 0 1.27-.33 1.63-.84L22 12l-4.37-6.16z"/>
</svg>
      </span>&nbsp;<span class="tag-name">CNN</span>
    </a>
  
</div>
  </div>

  
  <section class="related">
  <h2>Related Posts</h2>
  <ul class="posts-list">
    
      <li>
        <h3>
          <a href="/computer%20vision/2021/09/06/Pytorch-Computer-Vision-Ch02.html">
            Pytorch Computer Vision Ch02
            <small>06 Sep 2021</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2021/08/02/python_class.html">
            Python 클래스 개념
            <small>02 Aug 2021</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/computer%20vision/2021/07/20/Pytorch-Computer-Vision-Ch01.html">
            Pytorch Computer Vision Ch01
            <small>20 Jul 2021</small>
          </a>
        </h3>
      </li>
    
  </ul>
</section>

</div>

    </main>

    <!-- Optional footer content -->

  </body>
</html>
